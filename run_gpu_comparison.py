#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
GPUåŠ é€ŸRLå¯¹æ¯”å®éªŒ - ä¸»è¿è¡Œè„šæœ¬
===============================
åŠŸèƒ½ï¼š
1. GPU/CPUè‡ªåŠ¨æ£€æµ‹
2. è‡ªåŠ¨è°ƒæ•´batch_size
3. å…ˆ1ä¸ªç§å­å¿«é€ŸéªŒè¯
4. å†5ä¸ªç§å­å®Œæ•´å®éªŒ
5. ç»¼åˆè¯„åˆ†é€‰æ‹©æœ€ä¼˜æ–¹æ³•

è¿è¡Œæ–¹å¼ï¼š
    python run_gpu_comparison.py              # å®Œæ•´å®éªŒ
    python run_gpu_comparison.py --quick      # ä»…å¿«é€ŸéªŒè¯
    python run_gpu_comparison.py --full-only  # è·³è¿‡å¿«é€ŸéªŒè¯ç›´æ¥å®Œæ•´å®éªŒ

Author: CDC Project
Date: 2026-01-21
"""

import sys
import os
import argparse
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)


def print_banner():
    """æ‰“å°å¯åŠ¨æ¨ªå¹…"""
    print("\n" + "="*70)
    print("ğŸš€ æŸ´æ²¹æœºæ§åˆ¶æ–¹æ³• GPUåŠ é€Ÿå¯¹æ¯”å®éªŒ")
    print("="*70)
    print(f"å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-"*70)


def check_environment():
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒ"""
    print("\nğŸ“‹ ç¯å¢ƒæ£€æŸ¥:")
    
    # Pythonç‰ˆæœ¬
    print(f"  Pythonç‰ˆæœ¬: {sys.version.split()[0]}")
    
    # PyTorch
    try:
        import torch
        print(f"  PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        # GPUæ£€æµ‹
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"  âœ… GPUå¯ç”¨: {gpu_name} ({gpu_mem:.1f} GB)")
            
            # è‡ªåŠ¨batch_sizeå»ºè®®
            if gpu_mem >= 8:
                batch_size = 512
            elif gpu_mem >= 4:
                batch_size = 256
            else:
                batch_size = 128
            print(f"  ğŸ“¦ æ¨èbatch_size: {batch_size}")
            
            device = 'cuda'
        else:
            print("  âš ï¸  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            batch_size = 64
            device = 'cpu'
            
    except ImportError:
        print("  âŒ PyTorchæœªå®‰è£…")
        return None, None, None
    
    # æ£€æŸ¥é¡¹ç›®æ¨¡å—
    try:
        from agents.rl_algorithms import get_algorithm, SAC
        print("  âœ… åŸºç¡€RLç®—æ³•æ¨¡å—æ­£å¸¸")
    except ImportError as e:
        print(f"  âŒ åŸºç¡€RLç®—æ³•æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    
    try:
        from agents.advanced_rl_algorithms import (
            get_advanced_algorithm, TDMPC2, MambaPolicy, DPMD
        )
        print("  âœ… 2024-2025æ–°ç®—æ³•æ¨¡å—æ­£å¸¸")
    except ImportError as e:
        print(f"  âŒ æ–°ç®—æ³•æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    
    try:
        from experiments.five_method_comparison import FiveMethodComparison, ExperimentConfig
        print("  âœ… å®éªŒæ¡†æ¶æ¨¡å—æ­£å¸¸")
    except ImportError as e:
        print(f"  âŒ å®éªŒæ¡†æ¶æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return None, None, None
    
    print("-"*70)
    return device, batch_size, torch


def run_quick_validation(config):
    """è¿è¡Œå¿«é€ŸéªŒè¯"""
    from experiments.five_method_comparison import FiveMethodComparison
    
    print("\n" + "="*70)
    print("ğŸ” å¿«é€ŸéªŒè¯ (å•ç§å­: seed=42)")
    print("="*70)
    
    experiment = FiveMethodComparison(config)
    # è®ºæ–‡æ­£å¼å¯¹æ¯”æ–¹æ³•ï¼šPID + DQN + SAC + TD-MPC2 + DPMD
    methods = ["PID", "DQN", "SAC", "TDMPC2", "DPMD"]
    
    results = experiment.run_quick_validation(methods)
    
    return results, experiment


def run_full_experiment(config, experiment=None):
    """è¿è¡Œå®Œæ•´å®éªŒ"""
    from experiments.five_method_comparison import FiveMethodComparison
    
    if experiment is None:
        experiment = FiveMethodComparison(config)
    
    print("\n" + "="*70)
    print("ğŸ”¬ å®Œæ•´å¯¹æ¯”å®éªŒ (5ä¸ªç§å­: 42, 123, 456, 789, 1024)")
    print("="*70)
    
    # è®ºæ–‡æ­£å¼å¯¹æ¯”æ–¹æ³•ï¼šPID + DQN + SAC + TD-MPC2 + DPMD
    methods = ["PID", "DQN", "SAC", "TDMPC2", "DPMD"]
    comparison = experiment.run_full_comparison(methods)
    
    return comparison


def generate_report(comparison, save_dir: str, experiment=None):
    """ç”Ÿæˆå®éªŒæŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨"""
    import json
    import numpy as np
    
    print("\n" + "="*70)
    print("ğŸ“Š ç”Ÿæˆå®éªŒæŠ¥å‘Šå’Œå¯è§†åŒ–")
    print("="*70)
    
    os.makedirs(save_dir, exist_ok=True)
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    if experiment is not None:
        print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        experiment.plot_comparison_results(comparison, save_dir)
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    report_lines = []
    report_lines.append("# æŸ´æ²¹æœºæ§åˆ¶æ–¹æ³•å¯¹æ¯”å®éªŒæŠ¥å‘Š")
    report_lines.append(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("\n## å®éªŒé…ç½®")
    report_lines.append("- è®­ç»ƒEpisodes: 400")
    report_lines.append("- è¯„ä¼°Episodes: 10")
    report_lines.append("- éšæœºç§å­: [42, 123, 456, 789, 1024]")
    
    report_lines.append("\n## å¯¹æ¯”æ–¹æ³•")
    report_lines.append("| æ–¹æ³• | å¹´ä»½ | ç±»å‹ |")
    report_lines.append("|------|------|------|")
    report_lines.append("| PID | - | ä¼ ç»Ÿæ§åˆ¶ |")
    report_lines.append("| SAC | 2018 | æœ€å¤§ç†µRL |")
    report_lines.append("| TD-MPC2 | 2024 | æ¨¡å‹é¢„æµ‹æ§åˆ¶ |")
    report_lines.append("| Mamba Policy | 2025 | çŠ¶æ€ç©ºé—´æ¨¡å‹ |")
    report_lines.append("| DPMD | 2025 | æ‰©æ•£ç­–ç•¥+é•œåƒä¸‹é™ |")
    
    report_lines.append("\n## å®éªŒç»“æœ")
    report_lines.append("| æ’å | æ–¹æ³• | ç»¼åˆè¯„åˆ† | è¾¾æ ‡ç‡ | å¹³å‡å¥–åŠ± | æ”¶æ•›Episode |")
    report_lines.append("|------|------|----------|--------|----------|-------------|")
    
    sorted_methods = sorted(comparison.rankings.items(), key=lambda x: x[1])
    for method, rank in sorted_methods:
        results = comparison.all_results.get(method, [])
        if not results:
            continue
        
        mean_accuracy = np.mean([r.accuracy_rate for r in results])
        mean_reward = np.mean([r.final_reward for r in results])
        mean_convergence = np.mean([r.convergence_episode for r in results])
        score = comparison.scores.get(method, 0) if comparison.scores else 0
        
        report_lines.append(
            f"| {rank} | {method} | {score:.2f} | "
            f"{mean_accuracy*100:.1f}% | {mean_reward:.2f} | {mean_convergence:.0f} |"
        )
    
    report_lines.append(f"\n## ç»“è®º")
    report_lines.append(f"\n**æ¨èæ–¹æ³•: {comparison.best_method}**")
    best_score = comparison.scores.get(comparison.best_method, 0) if comparison.scores else 0
    report_lines.append(f"\nç»¼åˆè¯„åˆ†: {best_score:.2f}")
    
    # å†™å…¥æŠ¥å‘Š
    report_path = os.path.join(save_dir, "experiment_report.md")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    print(f"  ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    return report_path


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='GPUåŠ é€ŸRLå¯¹æ¯”å®éªŒ')
    parser.add_argument('--quick', action='store_true', 
                       help='ä»…è¿è¡Œå¿«é€ŸéªŒè¯')
    parser.add_argument('--full-only', action='store_true',
                       help='è·³è¿‡å¿«é€ŸéªŒè¯ï¼Œç›´æ¥è¿è¡Œå®Œæ•´å®éªŒ')
    parser.add_argument('--episodes', type=int, default=500,
                       help='è®­ç»ƒepisodesæ•°é‡ (é»˜è®¤: 500, æœ€ç»ˆå®éªŒ)')
    parser.add_argument('--steps', type=int, default=200,
                       help='æ¯episodeæ­¥æ•° (é»˜è®¤: 200, ä¼˜åŒ–å)')
    parser.add_argument('--save-dir', type=str, default='results/comparison',
                       help='ç»“æœä¿å­˜ç›®å½•')
    args = parser.parse_args()
    
    # æ‰“å°æ¨ªå¹…
    print_banner()
    
    # ç¯å¢ƒæ£€æŸ¥
    device, batch_size, torch = check_environment()
    if device is None:
        print("\nâŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿PyTorchå·²å®‰è£…")
        return
    
    # åˆ›å»ºé…ç½®
    from experiments.five_method_comparison import ExperimentConfig
    
    config = ExperimentConfig(
        n_episodes=args.episodes,
        max_steps_per_episode=args.steps,
        device=device,
        batch_size=batch_size,
        save_dir=args.save_dir
    )
    
    print(f"\nğŸ“‹ å®éªŒé…ç½®:")
    print(f"  - Episodes: {config.n_episodes}")
    print(f"  - Batch Size: {config.batch_size}")
    print(f"  - Device: {config.device}")
    print(f"  - ä¿å­˜ç›®å½•: {config.save_dir}")
    
    start_time = time.time()
    
    # è¿è¡Œå®éªŒ
    if args.quick:
        # ä»…å¿«é€ŸéªŒè¯
        results, experiment = run_quick_validation(config)
        
        # ç”Ÿæˆå¿«é€ŸéªŒè¯çš„å¯è§†åŒ–å›¾è¡¨
        if results and experiment:
            from experiments.five_method_comparison import ComparisonResult
            # å°†å¿«é€ŸéªŒè¯ç»“æœè½¬æ¢ä¸ºComparisonResultæ ¼å¼
            quick_comparison = ComparisonResult(
                methods=list(results.keys()),
                all_results={m: [r] for m, r in results.items()},
                rankings={m: i+1 for i, (m, r) in enumerate(
                    sorted(results.items(), key=lambda x: -x[1].accuracy_rate)
                )},
                best_method=max(results.items(), key=lambda x: x[1].accuracy_rate)[0]
            )
            generate_report(quick_comparison, args.save_dir, experiment)
        
        # æ‰“å°æ¨è
        if results:
            best = max(results.items(), key=lambda x: x[1].accuracy_rate)
            print(f"\nğŸ† å¿«é€ŸéªŒè¯æ¨è: {best[0]}")
    
    elif args.full_only:
        # ç›´æ¥å®Œæ•´å®éªŒ
        experiment = None
        comparison = run_full_experiment(config)
        generate_report(comparison, args.save_dir, experiment)
        print(f"\nğŸ† æœ€ç»ˆæ¨èæ–¹æ³•: {comparison.best_method}")
    
    else:
        # é»˜è®¤ï¼šå…ˆå¿«é€ŸéªŒè¯ï¼Œå†å®Œæ•´å®éªŒ
        results, experiment = run_quick_validation(config)
        
        print("\n" + "-"*70)
        print("å¿«é€ŸéªŒè¯å®Œæˆ! ç»§ç»­å®Œæ•´å®éªŒ...")
        print("-"*70)
        
        comparison = run_full_experiment(config, experiment)
        generate_report(comparison, args.save_dir, experiment)
        print(f"\nğŸ† æœ€ç»ˆæ¨èæ–¹æ³•: {comparison.best_method}")
    
    # æ€»æ—¶é—´
    total_time = time.time() - start_time
    print(f"\nâ±ï¸  æ€»è¿è¡Œæ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")
    print("="*70)
    print("âœ… å®éªŒå®Œæˆ!")
    print("="*70)


if __name__ == "__main__":
    main()
