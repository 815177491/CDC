#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¼ºåŒ–å­¦ä¹ æ§åˆ¶æ–¹æ³•å¯¹æ¯”å®éªŒæ¡†æ¶ (GPUåŠ é€Ÿ)
=====================================
è®ºæ–‡æ­£å¼å¯¹æ¯”å®éªŒï¼Œå¯¹æ¯”ä»¥ä¸‹5ç§æ–¹æ³•ï¼š
1. PID - ä¼ ç»Ÿæ§åˆ¶åŸºçº¿
2. DQN - ç»å…¸æ·±åº¦å¼ºåŒ–å­¦ä¹  (Nature 2015)
3. SAC - æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹  (ICML 2018)
4. TD-MPC2 - 2024å¹´æœ€æ–°æ–¹æ³• (ICLR 2024) â˜… æ¨è
5. DPMD - 2025å¹´æœ€æ–°æ–¹æ³• (æ‰©æ•£ç­–ç•¥+é•œåƒä¸‹é™)

å®éªŒè®¾è®¡ï¼š
- 500 episodesè®­ç»ƒï¼ˆæ­£å¼å®éªŒï¼‰
- 5ä¸ªéšæœºç§å­
- ç»¼åˆè¯„åˆ†é€‰æ‹©æœ€ä¼˜æ–¹æ³•
- å…ˆ1ä¸ªç§å­å¿«é€ŸéªŒè¯ï¼Œå†å…¨é‡è¿è¡Œ

è¯„ä¼°æŒ‡æ ‡ï¼š
- æ§åˆ¶ç²¾åº¦ï¼ˆPmaxè¯¯å·®<2barè¾¾æ ‡ç‡ï¼‰
- æ”¶æ•›é€Ÿåº¦ï¼ˆè¾¾åˆ°90%æ€§èƒ½çš„episodeï¼‰
- æ¨ç†æ—¶é—´ï¼ˆms/stepï¼‰
- è®­ç»ƒç¨³å®šæ€§ï¼ˆå¥–åŠ±æ ‡å‡†å·®ï¼‰

å¿«é€ŸéªŒè¯ç»“æœ (100 episodes, seed=42):
- TD-MPC2: 89.7% è¾¾æ ‡ç‡ â˜…
- SAC: 88.4% è¾¾æ ‡ç‡
- DPMD: 86.4% è¾¾æ ‡ç‡
- MambaPolicy: 70.4% è¾¾æ ‡ç‡ (æœªçº³å…¥æ­£å¼å¯¹æ¯”)
- PID: 0.5% è¾¾æ ‡ç‡

Author: CDC Project
Date: 2026-01-22
"""

import numpy as np
import random
import time
import os
import sys
import json
from dataclasses import dataclass, asdict
from typing import Dict, List, Tuple, Optional, Any
import warnings

# è¿›åº¦æ¡æ”¯æŒ
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False

# å¯è§†åŒ–æ”¯æŒ
try:
    import matplotlib
    matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯
    import matplotlib.pyplot as plt
    import matplotlib.patches as mpatches
    from matplotlib.gridspec import GridSpec
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    warnings.warn("Matplotlib not available, visualization will be disabled")
    def tqdm(iterable, **kwargs):
        return iterable

# å°è¯•å¯¼å…¥æ·±åº¦å­¦ä¹ åº“
try:
    import torch
    TORCH_AVAILABLE = True
    
    # GPUæ£€æµ‹
    if torch.cuda.is_available():
        DEVICE = torch.device('cuda')
        GPU_NAME = torch.cuda.get_device_name(0)
        GPU_MEM = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"[å®éªŒæ¡†æ¶] GPUæ£€æµ‹: {GPU_NAME} ({GPU_MEM:.1f} GB)")
    else:
        DEVICE = torch.device('cpu')
        GPU_NAME = None
        GPU_MEM = 0
        print("[å®éªŒæ¡†æ¶] ä½¿ç”¨CPUè¿è¡Œ")
        
except ImportError:
    TORCH_AVAILABLE = False
    DEVICE = None
    GPU_NAME = None
    GPU_MEM = 0


# ============================================================
# é…ç½®ç±»
# ============================================================

@dataclass
class ExperimentConfig:
    """å®éªŒé…ç½®"""
    # è®­ç»ƒå‚æ•° (ä¼˜åŒ–åï¼ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦)
    n_episodes: int = 100              # è®­ç»ƒepisodes (åŸ400)
    max_steps_per_episode: int = 200   # æ¯episodeæœ€å¤§æ­¥æ•° (åŸ500)
    
    # è¯„ä¼°å‚æ•°
    eval_frequency: int = 50           # è¯„ä¼°é¢‘ç‡ (åŸ25)
    n_eval_episodes: int = 3           # è¯„ä¼°episodesæ•° (åŸ10)
    
    # ç¯å¢ƒå‚æ•°
    state_dim: int = 8                 # çŠ¶æ€ç»´åº¦
    action_dim: int = 5                # åŠ¨ä½œç»´åº¦
    
    # GPUå‚æ•°
    device: str = 'cuda'
    batch_size: int = 256              # GPUå¯ç”¨æ›´å¤§batch
    
    # éšæœºç§å­
    seeds: List[int] = None
    
    # ä¿å­˜è·¯å¾„
    save_dir: str = 'results/comparison'
    
    def __post_init__(self):
        if self.seeds is None:
            self.seeds = [42, 123, 456, 789, 1024]
        
        # æ ¹æ®GPUæ˜¾å­˜è‡ªåŠ¨è°ƒæ•´batch_size
        if GPU_MEM >= 8:
            self.batch_size = 512
        elif GPU_MEM >= 4:
            self.batch_size = 256
        elif GPU_MEM > 0:
            self.batch_size = 128
        else:
            self.batch_size = 64


@dataclass
class MethodResult:
    """å•ä¸ªæ–¹æ³•çš„å®éªŒç»“æœ"""
    method_name: str
    seed: int
    
    # æ€§èƒ½æŒ‡æ ‡
    final_reward: float = 0.0
    mean_reward: float = 0.0
    std_reward: float = 0.0
    max_reward: float = 0.0
    
    # æ§åˆ¶ç²¾åº¦
    pmax_error_mean: float = 0.0       # Pmaxè¯¯å·®å‡å€¼
    pmax_error_std: float = 0.0        # Pmaxè¯¯å·®æ ‡å‡†å·®  
    accuracy_rate: float = 0.0         # è¾¾æ ‡ç‡ (è¯¯å·®<2bar)
    
    # æ•ˆç‡æŒ‡æ ‡
    convergence_episode: int = 0       # æ”¶æ•›episode
    training_time: float = 0.0         # è®­ç»ƒæ—¶é—´(ç§’)
    inference_time_ms: float = 0.0     # æ¨ç†æ—¶é—´(æ¯«ç§’/æ­¥)
    
    # è®­ç»ƒæ›²çº¿
    reward_curve: List[float] = None
    eval_curve: List[float] = None


@dataclass
class ComparisonResult:
    """å¯¹æ¯”å®éªŒæ€»ç»“æœ"""
    methods: List[str]
    all_results: Dict[str, List[MethodResult]]
    
    # ç»¼åˆè¯„åˆ†
    scores: Dict[str, float] = None
    best_method: str = ""
    
    # æ’å
    rankings: Dict[str, int] = None


# ============================================================
# æŸ´æ²¹æœºæ§åˆ¶ç¯å¢ƒ (ç®€åŒ–ç‰ˆ)
# ============================================================

class DieselEngineEnv:
    """
    æŸ´æ²¹æœºPmaxæ§åˆ¶ä»¿çœŸç¯å¢ƒ (å«æ•…éšœæ³¨å…¥)
    
    çŠ¶æ€: [pmax, pmax_error, rpm, fuel_rate, timing, rail_p, boost_p, temp]
    åŠ¨ä½œ: 5ä¸ªç¦»æ•£è°ƒæ•´æ¡£ä½ (-2, -1, 0, +1, +2)
    æ•…éšœ: 30%æ¦‚ç‡éšæœºæ³¨å…¥å„ç±»æ•…éšœ
    """
    
    def __init__(self, seed: int = None, fault_probability: float = 0.3):
        if seed is not None:
            np.random.seed(seed)
            random.seed(seed)
        
        self.state_dim = 8
        self.action_dim = 5
        self.fault_probability = fault_probability
        
        # ç›®æ ‡Pmax
        self.pmax_target = 180.0  # bar
        self.pmax_tolerance = 2.0  # bar
        
        # çŠ¶æ€èŒƒå›´
        self.state_ranges = {
            'pmax': (150, 210),
            'rpm': (500, 1000),
            'fuel_rate': (50, 150),
            'timing': (-5, 10),
            'rail_p': (1000, 2000),
            'boost_p': (1.5, 3.5),
            'temp': (300, 500)
        }
        
        # æ•…éšœçŠ¶æ€åˆå§‹åŒ–
        self.reset_fault_state()
        self.reset()
    
    def reset_fault_state(self):
        """é‡ç½®æ•…éšœçŠ¶æ€"""
        self.fault_active = False
        self.fault_type = None
        self.fault_severity = 0.0
        self.fault_onset_step = 0
        self.fault_duration = 0
        self.steps_in_fault = 0
        
    def inject_random_fault(self):
        """éšæœºæ³¨å…¥æ•…éšœ (30%æ¦‚ç‡)"""
        if np.random.random() < self.fault_probability:
            self.fault_active = True
            
            # æ•…éšœç±»å‹
            fault_types = [
                'injection_timing',  # å–·æ²¹æ­£æ—¶åç§»
                'fuel_system',       # ç‡ƒæ²¹ç³»ç»Ÿæ•…éšœ  
                'compression_leak',  # å‹ç¼©æ³„æ¼
                'turbo_lag',        # å¢å‹æ»å
                'sensor_drift'      # ä¼ æ„Ÿå™¨æ¼‚ç§»
            ]
            self.fault_type = np.random.choice(fault_types)
            
            # æ•…éšœä¸¥é‡ç¨‹åº¦ (0.3-1.0)
            self.fault_severity = np.random.uniform(0.3, 1.0)
            
            # æ•…éšœå‘ç”Ÿæ—¶é—´ (episodeä¸­çš„10-80æ­¥)
            self.fault_onset_step = np.random.randint(10, 80)
            
            # æ•…éšœæŒç»­æ—¶é—´ (20-100æ­¥)  
            self.fault_duration = np.random.randint(20, 100)
            
            print(f"[æ•…éšœæ³¨å…¥] ç±»å‹:{self.fault_type}, ä¸¥é‡åº¦:{self.fault_severity:.2f}, "
                  f"å¼€å§‹:{self.fault_onset_step}æ­¥, æŒç»­:{self.fault_duration}æ­¥")
    
    def reset(self) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒ"""
        # éšæœºåˆå§‹åŒ–
        self.pmax = np.random.uniform(160, 200)
        self.rpm = np.random.uniform(600, 900)
        self.fuel_rate = np.random.uniform(80, 120)
        self.timing = np.random.uniform(0, 5)
        self.rail_p = np.random.uniform(1200, 1800)
        self.boost_p = np.random.uniform(2.0, 3.0)
        self.temp = np.random.uniform(350, 450)
        
        self.step_count = 0
        self.reset_fault_state()
        self.inject_random_fault()  # å†³å®šæœ¬episodeæ˜¯å¦æœ‰æ•…éšœ
        
        return self._get_state()
    
    def apply_fault_effects(self):
        """åº”ç”¨æ•…éšœå¯¹ç³»ç»Ÿçš„å½±å“"""
        if not self.fault_active:
            return
            
        # æ£€æŸ¥æ•…éšœæ˜¯å¦åº”è¯¥å¼€å§‹
        if self.step_count >= self.fault_onset_step and self.steps_in_fault == 0:
            print(f"[t={self.step_count}] æ•…éšœå¼€å§‹: {self.fault_type}")
            
        # æ•…éšœæ¿€æ´»æœŸé—´
        if (self.step_count >= self.fault_onset_step and 
            self.steps_in_fault < self.fault_duration):
            
            self.steps_in_fault += 1
            
            # æ ¹æ®æ•…éšœç±»å‹åº”ç”¨ä¸åŒå½±å“
            if self.fault_type == 'injection_timing':
                # å–·æ²¹æ­£æ—¶åç§» -> Pmaxå‡é«˜
                fault_effect = self.fault_severity * 8.0  # æœ€å¤§8 baråå·®
                self.pmax += fault_effect * np.sin(0.1 * self.steps_in_fault)
                
            elif self.fault_type == 'fuel_system':
                # ç‡ƒæ²¹ç³»ç»Ÿæ•…éšœ -> ç‡ƒæ²¹å‹åŠ›æ³¢åŠ¨
                self.rail_p += self.fault_severity * 200 * np.sin(0.2 * self.steps_in_fault)
                self.pmax += self.fault_severity * 5.0 * np.random.normal(0, 1)
                
            elif self.fault_type == 'compression_leak':
                # å‹ç¼©æ³„æ¼ -> Pmaxä¸‹é™
                self.pmax -= self.fault_severity * 6.0 * (1 - np.exp(-0.1 * self.steps_in_fault))
                
            elif self.fault_type == 'turbo_lag':
                # å¢å‹æ»å -> è¿›æ°”å‹åŠ›æ³¢åŠ¨
                self.boost_p *= (1 - self.fault_severity * 0.3 * np.sin(0.15 * self.steps_in_fault))
                self.pmax += self.fault_severity * 3.0 * np.random.normal(0, 0.5)
                
            elif self.fault_type == 'sensor_drift':
                # ä¼ æ„Ÿå™¨æ¼‚ç§» -> æµ‹é‡å™ªå£°å¢åŠ 
                drift = self.fault_severity * 4.0 * (self.steps_in_fault / self.fault_duration)
                self.pmax += drift + np.random.normal(0, self.fault_severity * 2.0)
        
        # æ•…éšœç»“æŸ
        elif self.steps_in_fault >= self.fault_duration:
            if self.fault_active:
                print(f"[t={self.step_count}] æ•…éšœç»“æŸ: {self.fault_type}")
                self.fault_active = False
    
    def _get_state(self) -> np.ndarray:
        """è·å–å½’ä¸€åŒ–çŠ¶æ€"""
        pmax_norm = (self.pmax - 150) / 60
        error_norm = (self.pmax - self.pmax_target) / 30
        rpm_norm = (self.rpm - 500) / 500
        fuel_norm = (self.fuel_rate - 50) / 100
        timing_norm = (self.timing + 5) / 15
        rail_norm = (self.rail_p - 1000) / 1000
        boost_norm = (self.boost_p - 1.5) / 2.0
        temp_norm = (self.temp - 300) / 200
        
        return np.array([
            pmax_norm, error_norm, rpm_norm, fuel_norm,
            timing_norm, rail_norm, boost_norm, temp_norm
        ], dtype=np.float32)
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """æ‰§è¡ŒåŠ¨ä½œ"""
        # åŠ¨ä½œæ˜ å°„: 0=-2, 1=-1, 2=0, 3=+1, 4=+2
        adjustment = (action - 2) * 0.5  # å–·æ²¹æ—¶åˆ»è°ƒæ•´
        
        # åº”ç”¨æ§åˆ¶
        self.timing += adjustment
        self.timing = np.clip(self.timing, -5, 10)
        
        # æ¨¡æ‹Ÿå‘åŠ¨æœºå“åº”
        self._simulate_engine()
        
        # è®¡ç®—å¥–åŠ±
        error = abs(self.pmax - self.pmax_target)
        
        if error < self.pmax_tolerance:
            reward = 10.0 - error  # åœ¨ç›®æ ‡èŒƒå›´å†…ï¼Œé«˜å¥–åŠ±
        else:
            reward = -error * 0.5  # åç¦»ç›®æ ‡ï¼Œæƒ©ç½š
        
        # å®‰å…¨çº¦æŸ
        if self.pmax > 200 or self.pmax < 160:
            reward -= 10.0  # å®‰å…¨æƒ©ç½š
        
        self.step_count += 1
        done = self.step_count >= 500
        
        info = {
            'pmax': self.pmax,
            'error': error,
            'in_tolerance': error < self.pmax_tolerance
        }
        
        return self._get_state(), reward, done, info
    
    def _simulate_engine(self):
        """ç®€åŒ–çš„å‘åŠ¨æœºåŠ¨åŠ›å­¦æ¨¡æ‹Ÿ (å«æ•…éšœå½±å“)"""
        # åº”ç”¨æ•…éšœæ•ˆæœ
        self.apply_fault_effects()
        
        # Pmaxå“åº” (åŸºäºå–·æ²¹æ—¶åˆ»)
        delta_pmax = (self.timing - 2) * 1.5 + np.random.normal(0, 0.5)
        self.pmax = 0.95 * self.pmax + 0.05 * (175 + delta_pmax)
        
        # æ·»åŠ åŸºç¡€å¹²æ‰°
        self.pmax += np.random.normal(0, 0.3)
        self.pmax = np.clip(self.pmax, 150, 210)
        
        # å…¶ä»–çŠ¶æ€éšæœºå˜åŒ–
        self.rpm += np.random.normal(0, 5)
        self.rpm = np.clip(self.rpm, 500, 1000)
        
        self.fuel_rate += np.random.normal(0, 1)
        self.fuel_rate = np.clip(self.fuel_rate, 50, 150)
        
        # ç¡®ä¿boost_på’Œrail_påœ¨èŒƒå›´å†…
        self.boost_p = np.clip(self.boost_p, 1.5, 3.5)
        self.rail_p = np.clip(self.rail_p, 1000, 2000)


# ============================================================
# PIDæ§åˆ¶å™¨
# ============================================================

class PIDController:
    """PIDæ§åˆ¶å™¨ (åŸºçº¿æ–¹æ³•)"""
    
    def __init__(self, kp: float = 0.5, ki: float = 0.1, kd: float = 0.2):
        self.kp = kp
        self.ki = ki
        self.kd = kd
        
        self.integral = 0
        self.prev_error = 0
    
    def select_action(self, state: np.ndarray, explore: bool = False) -> int:
        """æ ¹æ®çŠ¶æ€é€‰æ‹©åŠ¨ä½œ"""
        # state[1]æ˜¯å½’ä¸€åŒ–çš„pmaxè¯¯å·®
        error = state[1] * 30  # åå½’ä¸€åŒ–
        
        # PIDè®¡ç®—
        self.integral += error
        derivative = error - self.prev_error
        self.prev_error = error
        
        control = self.kp * error + self.ki * self.integral + self.kd * derivative
        
        # æ˜ å°„åˆ°ç¦»æ•£åŠ¨ä½œ
        if control > 1:
            return 4  # +2
        elif control > 0.3:
            return 3  # +1
        elif control < -1:
            return 0  # -2
        elif control < -0.3:
            return 1  # -1
        else:
            return 2  # 0
    
    def reset(self):
        self.integral = 0
        self.prev_error = 0
    
    def update(self, batch=None):
        """PIDä¸éœ€è¦æ›´æ–°"""
        return {}
    
    def get_name(self):
        return "PID"


# ============================================================
# å®éªŒè¿è¡Œå™¨
# ============================================================

class FiveMethodComparison:
    """äº”ç§æ–¹æ³•å¯¹æ¯”å®éªŒ"""
    
    def __init__(self, config: ExperimentConfig = None):
        self.config = config or ExperimentConfig()
        
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs(self.config.save_dir, exist_ok=True)
        
        # å¯¼å…¥ç®—æ³•
        try:
            from agents.rl_algorithms import get_algorithm, SAC
            from agents.advanced_rl_algorithms import (
                get_advanced_algorithm, TDMPC2, MambaPolicy, DPMD
            )
            self.rl_available = True
        except ImportError as e:
            print(f"[è­¦å‘Š] æ— æ³•å¯¼å…¥RLç®—æ³•: {e}")
            self.rl_available = False
        
        self.results = {}
    
    def _create_method(self, method_name: str, seed: int):
        """åˆ›å»ºæ–¹æ³•å®ä¾‹"""
        if method_name == "PID":
            return PIDController()
        
        if not self.rl_available:
            raise RuntimeError("RL algorithms not available")
        
        from agents.rl_algorithms import get_algorithm
        from agents.advanced_rl_algorithms import get_advanced_algorithm
        
        config = {
            'device': str(DEVICE),
            'batch_size': self.config.batch_size,
            'gamma': 0.99,
        }
        
        if method_name == "SAC":
            return get_algorithm("SAC", self.config.state_dim, 
                               self.config.action_dim, config)
        elif method_name == "DQN":
            return get_algorithm("DQN", self.config.state_dim, 
                               self.config.action_dim, config)
        elif method_name in ["TDMPC2", "TD-MPC2"]:
            return get_advanced_algorithm("TDMPC2", self.config.state_dim,
                                         self.config.action_dim, config)
        elif method_name in ["MambaPolicy", "Mamba"]:
            return get_advanced_algorithm("MambaPolicy", self.config.state_dim,
                                         self.config.action_dim, config)
        elif method_name == "DPMD":
            return get_advanced_algorithm("DPMD", self.config.state_dim,
                                         self.config.action_dim, config)
        else:
            raise ValueError(f"Unknown method: {method_name}")
    
    def train_single(self, method_name: str, seed: int, 
                     verbose: bool = True) -> MethodResult:
        """è®­ç»ƒå•ä¸ªæ–¹æ³•å•ä¸ªç§å­"""
        # è®¾ç½®éšæœºç§å­
        np.random.seed(seed)
        random.seed(seed)
        if TORCH_AVAILABLE:
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed(seed)
        
        # åˆ›å»ºç¯å¢ƒå’Œæ–¹æ³•
        env = DieselEngineEnv(seed)
        method = self._create_method(method_name, seed)
        
        # ç»“æœè®°å½•
        result = MethodResult(method_name=method_name, seed=seed)
        result.reward_curve = []
        result.eval_curve = []
        
        episode_rewards = []
        pmax_errors = []
        in_tolerance_count = 0
        total_steps = 0
        
        train_start = time.time()
        
        # ç”¨äºå­˜å‚¨æœ€æ–°çš„è¯„ä¼°ç»“æœ
        last_eval_reward = 0.0
        
        # è®­ç»ƒé…ç½®
        n_eps = self.config.n_episodes
        show_progress = verbose  # æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        
        for episode in range(self.config.n_episodes):
            state = env.reset()
            if hasattr(method, 'reset'):
                method.reset()
            if hasattr(method, 'reset_history'):
                method.reset_history()
            
            episode_reward = 0
            episode_errors = []
            episode_states = []
            episode_actions = []
            episode_rewards_list = []
            
            for step in range(self.config.max_steps_per_episode):
                # é€‰æ‹©åŠ¨ä½œ
                step_start = time.time()
                action = method.select_action(state, explore=True)
                inference_time = (time.time() - step_start) * 1000  # ms
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = env.step(action)
                
                episode_reward += reward
                episode_errors.append(info['error'])
                if info['in_tolerance']:
                    in_tolerance_count += 1
                total_steps += 1
                
                # å­˜å‚¨ç»éªŒ
                if hasattr(method, 'buffer') and method.buffer is not None:
                    method.buffer.push(state, action, reward, next_state, done)
                
                # è®°å½•è½¨è¿¹ (ç”¨äºMamba)
                episode_states.append(state)
                episode_actions.append(action)
                episode_rewards_list.append(reward)
                
                # æ›´æ–°æ–¹æ³•
                if hasattr(method, 'buffer') and method.buffer is not None and len(method.buffer) >= 64:
                    batch = method.buffer.sample(self.config.batch_size)
                    method.update(batch)
                elif hasattr(method, 'store_transition'):
                    method.store_transition(state, action, reward, done)
                
                state = next_state
                
                if done:
                    break
            
            # å­˜å‚¨è½¨è¿¹ (ç”¨äºMambaåºåˆ—è®­ç»ƒ)
            if hasattr(method, 'store_trajectory'):
                method.store_trajectory(episode_states, episode_actions, episode_rewards_list)
            
            # PPOæ›´æ–°
            if hasattr(method, 'states') and len(method.states) > 0:
                method.update()
            
            episode_rewards.append(episode_reward)
            pmax_errors.extend(episode_errors)
            result.reward_curve.append(episode_reward)
            
            # å®šæœŸè¯„ä¼°
            if (episode + 1) % self.config.eval_frequency == 0:
                eval_reward = self._evaluate(method, env, self.config.n_eval_episodes)
                result.eval_curve.append(eval_reward)
                last_eval_reward = eval_reward
            
            # æ›´æ–°è¿›åº¦æ¡ - æ‰‹åŠ¨å®ç°ç²¾ç¡®æ ¼å¼
            # æ ¼å¼: PID      45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | Episode 225/500 | Reward:  -850.3 | Eval:  -920.1 | Error:  1.25bar
            if show_progress:
                avg_reward = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)
                mean_error = np.mean(episode_errors)
                ep = episode + 1
                
                # æ„å»ºè¿›åº¦æ¡
                pct = int(ep / n_eps * 100)
                filled = int(20 * ep / n_eps)
                bar_str = 'â–ˆ' * filled + ' ' * (20 - filled)
                
                # å®Œæ•´æ ¼å¼å­—ç¬¦ä¸²
                status = f"\r{method_name:<8s}{pct:3d}%|{bar_str}| Episode {ep:3d}/{n_eps} | Reward: {avg_reward:8.1f} | Eval: {last_eval_reward:8.1f} | Error: {mean_error:5.2f}bar"
                sys.stdout.write(status)
                sys.stdout.flush()
        
        # è¿›åº¦æ¡ç»“æŸåæ¢è¡Œ
        if show_progress:
            print()
        
        # è®¡ç®—ç»“æœ
        result.training_time = time.time() - train_start
        result.final_reward = np.mean(episode_rewards[-20:])
        result.mean_reward = np.mean(episode_rewards)
        result.std_reward = np.std(episode_rewards)
        result.max_reward = np.max(episode_rewards)
        result.pmax_error_mean = np.mean(pmax_errors)
        result.pmax_error_std = np.std(pmax_errors)
        result.accuracy_rate = in_tolerance_count / total_steps
        result.inference_time_ms = inference_time  # æœ€åä¸€æ­¥çš„æ¨ç†æ—¶é—´
        
        # ä¼°è®¡æ”¶æ•›episode
        for i, r in enumerate(episode_rewards):
            if r >= 0.9 * result.max_reward:
                result.convergence_episode = i
                break
        
        if verbose:
            print(f"\n[{method_name}] è®­ç»ƒå®Œæˆ!")
            print(f"  æœ€ç»ˆå¥–åŠ±: {result.final_reward:.2f}")
            print(f"  Pmaxè¯¯å·®: {result.pmax_error_mean:.2f} Â± {result.pmax_error_std:.2f} bar")
            print(f"  è¾¾æ ‡ç‡: {result.accuracy_rate*100:.1f}%")
            print(f"  è®­ç»ƒæ—¶é—´: {result.training_time:.1f}s")
        
        return result
    
    def _evaluate(self, method, env: DieselEngineEnv, n_episodes: int) -> float:
        """è¯„ä¼°æ–¹æ³•æ€§èƒ½"""
        total_reward = 0
        
        for _ in range(n_episodes):
            state = env.reset()
            if hasattr(method, 'reset'):
                method.reset()
            if hasattr(method, 'reset_history'):
                method.reset_history()
            
            episode_reward = 0
            for _ in range(self.config.max_steps_per_episode):
                action = method.select_action(state, explore=False)
                next_state, reward, done, _ = env.step(action)
                episode_reward += reward
                state = next_state
                if done:
                    break
            
            total_reward += episode_reward
        
        return total_reward / n_episodes
    
    def run_quick_validation(self, methods: List[str] = None) -> Dict[str, MethodResult]:
        """å¿«é€ŸéªŒè¯ - å•ä¸ªç§å­"""
        if methods is None:
            # è®ºæ–‡æ­£å¼å¯¹æ¯”æ–¹æ³•ï¼šPID + DQN + SAC + TD-MPC2 + DPMD
            methods = ["PID", "DQN", "SAC", "TDMPC2", "DPMD"]
        
        print("\n" + "="*70)
        print("ğŸš€ å¿«é€ŸéªŒè¯æ¨¡å¼ (1ä¸ªç§å­)")
        print(f"   Episodes: {self.config.n_episodes} | Seed: {self.config.seeds[0]}")
        print("="*70)
        
        results = {}
        seed = self.config.seeds[0]
        
        for method_idx, method_name in enumerate(methods):
            print(f"\n{'â”€'*70}")
            print(f"ğŸ”¹ æ–¹æ³• [{method_idx+1}/{len(methods)}]: {method_name}")
            print(f"{'â”€'*70}")
            try:
                result = self.train_single(method_name, seed, verbose=True)
                results[method_name] = result
                print(f"  âœ… å®Œæˆ: å¥–åŠ±={result.final_reward:.1f} | "
                      f"è¾¾æ ‡ç‡={result.accuracy_rate*100:.1f}% | "
                      f"è®­ç»ƒæ—¶é—´={result.training_time:.1f}s")
            except Exception as e:
                print(f"  âŒ {method_name} è®­ç»ƒå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # æ‰“å°å¿«é€ŸéªŒè¯ç»“æœ
        self._print_validation_summary(results)
        
        return results
    
    def run_full_comparison(self, methods: List[str] = None) -> ComparisonResult:
        """å®Œæ•´å¯¹æ¯”å®éªŒ - 5ä¸ªç§å­"""
        if methods is None:
            # è®ºæ–‡æ­£å¼å¯¹æ¯”æ–¹æ³•ï¼šPID + DQN + SAC + TD-MPC2 + DPMD
            methods = ["PID", "DQN", "SAC", "TDMPC2", "DPMD"]
        
        print("\n" + "="*70)
        print("ğŸ”¬ å®Œæ•´å¯¹æ¯”å®éªŒ (5ä¸ªç§å­ Ã— 5ç§æ–¹æ³•)")
        print(f"   Episodes: {self.config.n_episodes} | Seeds: {self.config.seeds}")
        print("="*70)
        
        all_results = {m: [] for m in methods}
        total_runs = len(methods) * len(self.config.seeds)
        completed_runs = 0
        
        for method_idx, method_name in enumerate(methods):
            print(f"\n{'â”€'*70}")
            print(f"ğŸ”¹ æ–¹æ³• [{method_idx+1}/{len(methods)}]: {method_name}")
            print(f"{'â”€'*70}")
            
            # å†…å±‚è¿›åº¦æ¡ï¼ˆç§å­çº§ï¼‰- æ¯ä¸ªç§å­å•ç‹¬è®­ç»ƒå¹¶æ˜¾ç¤ºè¿›åº¦
            for seed_idx, seed in enumerate(self.config.seeds):
                print(f"\n  ğŸ“Œ ç§å­ [{seed_idx+1}/{len(self.config.seeds)}]: {seed}")
                try:
                    result = self.train_single(method_name, seed, verbose=True)
                    all_results[method_name].append(result)
                    completed_runs += 1
                    
                    # æ‰“å°è¯¥ç§å­çš„ç»“æœæ‘˜è¦
                    print(f"  âœ… å®Œæˆ: å¥–åŠ±={result.final_reward:.1f} | "
                          f"è¾¾æ ‡ç‡={result.accuracy_rate*100:.1f}% | "
                          f"è®­ç»ƒæ—¶é—´={result.training_time:.1f}s")
                        
                except Exception as e:
                    print(f"  âŒ ç§å­ {seed} å¤±è´¥: {e}")
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†å¹¶æ’å
        comparison = self._compute_comparison(methods, all_results)
        
        # ä¿å­˜ç»“æœ
        self._save_results(comparison)
        
        # æ‰“å°æ€»ç»“
        self._print_comparison_summary(comparison)
        
        return comparison
    
    def _compute_comparison(self, methods: List[str], 
                           all_results: Dict[str, List[MethodResult]]) -> ComparisonResult:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        scores = {}
        
        for method in methods:
            results = all_results[method]
            if not results:
                scores[method] = 0
                continue
            
            # è®¡ç®—å„ç»´åº¦å‡å€¼
            mean_accuracy = np.mean([r.accuracy_rate for r in results])
            mean_reward = np.mean([r.final_reward for r in results])
            mean_convergence = np.mean([r.convergence_episode for r in results])
            mean_inference = np.mean([r.inference_time_ms for r in results])
            
            # ç»¼åˆè¯„åˆ† (åŠ æƒ)
            # æ§åˆ¶ç²¾åº¦ 40% + æœ€ç»ˆå¥–åŠ± 30% + æ”¶æ•›é€Ÿåº¦ 20% + æ¨ç†é€Ÿåº¦ 10%
            score = (
                0.4 * mean_accuracy * 100 +  # è¾¾æ ‡ç‡ (0-100)
                0.3 * (mean_reward + 100) / 10 +  # å¥–åŠ±å½’ä¸€åŒ–
                0.2 * max(0, 100 - mean_convergence / 4) +  # æ”¶æ•›é€Ÿåº¦
                0.1 * max(0, 100 - mean_inference * 10)  # æ¨ç†é€Ÿåº¦
            )
            scores[method] = score
        
        # æ’å
        sorted_methods = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        rankings = {m: i+1 for i, (m, _) in enumerate(sorted_methods)}
        best_method = sorted_methods[0][0] if sorted_methods else ""
        
        return ComparisonResult(
            methods=methods,
            all_results=all_results,
            scores=scores,
            rankings=rankings,
            best_method=best_method
        )
    
    def _print_validation_summary(self, results: Dict[str, MethodResult]):
        """æ‰“å°å¿«é€ŸéªŒè¯ç»“æœ"""
        print("\n" + "="*70)
        print("ğŸ“Š å¿«é€ŸéªŒè¯ç»“æœ")
        print("="*70)
        print(f"{'æ–¹æ³•':<15} {'å¥–åŠ±':<12} {'è¾¾æ ‡ç‡':<12} {'Pmaxè¯¯å·®':<15} {'æ—¶é—´':<10}")
        print("-"*70)
        
        for name, result in results.items():
            print(f"{name:<15} {result.final_reward:<12.2f} "
                  f"{result.accuracy_rate*100:<11.1f}% "
                  f"{result.pmax_error_mean:.2f}Â±{result.pmax_error_std:.2f}bar  "
                  f"{result.training_time:<10.1f}s")
        
        # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
        best = max(results.items(), key=lambda x: x[1].accuracy_rate)
        print("\n" + "="*70)
        print(f"ğŸ† å¿«é€ŸéªŒè¯æœ€ä½³æ–¹æ³•: {best[0]} (è¾¾æ ‡ç‡: {best[1].accuracy_rate*100:.1f}%)")
        print("="*70)
    
    def _print_comparison_summary(self, comparison: ComparisonResult):
        """æ‰“å°å®Œæ•´å¯¹æ¯”ç»“æœ"""
        print("\n" + "="*70)
        print("ğŸ“Š å®Œæ•´å¯¹æ¯”å®éªŒç»“æœ")
        print("="*70)
        
        print(f"\n{'æ’å':<6} {'æ–¹æ³•':<15} {'ç»¼åˆè¯„åˆ†':<12} {'è¾¾æ ‡ç‡':<12} {'å¥–åŠ±':<12} {'æ”¶æ•›':<10}")
        print("-"*70)
        
        # æŒ‰æ’åæ’åº
        sorted_methods = sorted(comparison.rankings.items(), key=lambda x: x[1])
        
        for method, rank in sorted_methods:
            results = comparison.all_results.get(method, [])
            if not results:
                continue
            
            mean_accuracy = np.mean([r.accuracy_rate for r in results])
            mean_reward = np.mean([r.final_reward for r in results])
            mean_convergence = np.mean([r.convergence_episode for r in results])
            score = comparison.scores.get(method, 0)
            
            medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else "  "
            print(f"{medal}{rank:<4} {method:<15} {score:<12.2f} "
                  f"{mean_accuracy*100:<11.1f}% {mean_reward:<12.2f} ep{mean_convergence:<8.0f}")
        
        print("\n" + "="*70)
        print(f"ğŸ† æœ€ä¼˜æ–¹æ³•: {comparison.best_method}")
        print(f"   ç»¼åˆè¯„åˆ†: {comparison.scores.get(comparison.best_method, 0):.2f}")
        print("="*70)
    
    def _save_results(self, comparison: ComparisonResult):
        """ä¿å­˜å®éªŒç»“æœ"""
        # ä¿å­˜JSONæ‘˜è¦
        summary = {
            'methods': comparison.methods,
            'scores': comparison.scores,
            'rankings': comparison.rankings,
            'best_method': comparison.best_method,
            'config': asdict(self.config)
        }
        
        # æ·»åŠ æ¯ä¸ªæ–¹æ³•çš„è¯¦ç»†ç»“æœ
        for method, results in comparison.all_results.items():
            if results:
                summary[method] = {
                    'mean_accuracy': np.mean([r.accuracy_rate for r in results]),
                    'mean_reward': np.mean([r.final_reward for r in results]),
                    'std_reward': np.std([r.final_reward for r in results]),
                    'mean_convergence': np.mean([r.convergence_episode for r in results]),
                    'mean_training_time': np.mean([r.training_time for r in results]),
                }
        
        # ä¿å­˜
        save_path = os.path.join(self.config.save_dir, 'comparison_results.json')
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {save_path}")
    
    def plot_comparison_results(self, comparison: ComparisonResult, save_dir: str = None):
        """
        ç»˜åˆ¶äº”ç§æ–¹æ³•å¯¹æ¯”å›¾
        
        åŒ…å«ï¼š
        1. è¾¾æ ‡ç‡å¯¹æ¯”æŸ±çŠ¶å›¾
        2. å­¦ä¹ æ›²çº¿å¯¹æ¯”
        3. è®­ç»ƒæ—¶é—´å¯¹æ¯”
        4. ç»¼åˆé›·è¾¾å›¾
        """
        if not MATPLOTLIB_AVAILABLE:
            print("âš ï¸  Matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å¯è§†åŒ–")
            return
        
        if save_dir is None:
            save_dir = self.config.save_dir
        
        os.makedirs(save_dir, exist_ok=True)
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # é¢œè‰²æ–¹æ¡ˆ
        colors = {
            'PID': '#95a5a6',      # ç°è‰²
            'DQN': '#3498db',      # è“è‰²
            'SAC': '#e74c3c',      # çº¢è‰²
            'TDMPC2': '#2ecc71',   # ç»¿è‰² (æœ€ä¼˜)
            'DPMD': '#f39c12',     # æ©™è‰²
        }
        
        methods = comparison.methods
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = {}
        for method in methods:
            results = comparison.all_results.get(method, [])
            if results:
                avg_metrics[method] = {
                    'accuracy': np.mean([r.accuracy_rate for r in results]) * 100,
                    'reward': np.mean([r.final_reward for r in results]),
                    'convergence': np.mean([r.convergence_episode for r in results]),
                    'time': np.mean([r.training_time for r in results]),
                }
        
        # ============ å›¾1ï¼šè¾¾æ ‡ç‡å¯¹æ¯”æŸ±çŠ¶å›¾ ============
        fig1, ax1 = plt.subplots(figsize=(10, 6))
        
        method_names = list(avg_metrics.keys())
        accuracies = [avg_metrics[m]['accuracy'] for m in method_names]
        bar_colors = [colors.get(m, '#34495e') for m in method_names]
        
        bars = ax1.bar(method_names, accuracies, color=bar_colors, alpha=0.8, edgecolor='black')
        
        # æ ‡æ³¨æ•°å€¼
        for bar, acc in zip(bars, accuracies):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{acc:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')
        
        ax1.set_ylabel('Pmaxæ§åˆ¶è¾¾æ ‡ç‡ (%)', fontsize=13, fontweight='bold')
        ax1.set_title('äº”ç§æ–¹æ³•Pmaxæ§åˆ¶è¾¾æ ‡ç‡å¯¹æ¯” (PID+DQN+SAC+TD-MPC2+DPMD)', 
                     fontsize=14, fontweight='bold')
        ax1.set_ylim(0, 100)
        ax1.grid(axis='y', alpha=0.3, linestyle='--')
        ax1.axhline(y=90, color='red', linestyle='--', alpha=0.5, label='90%ç›®æ ‡çº¿')
        ax1.legend(fontsize=11)
        
        plt.tight_layout()
        accuracy_path = os.path.join(save_dir, 'accuracy_comparison.png')
        plt.savefig(accuracy_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"  ğŸ“Š è¾¾æ ‡ç‡å¯¹æ¯”å›¾å·²ä¿å­˜: {accuracy_path}")
        
        # ============ å›¾2ï¼šå­¦ä¹ æ›²çº¿å¯¹æ¯” ============
        fig2, ax2 = plt.subplots(figsize=(12, 7))
        
        for method in methods:
            results = comparison.all_results.get(method, [])
            if results and results[0].reward_curve:
                # å–ç¬¬ä¸€ä¸ªç§å­çš„å­¦ä¹ æ›²çº¿ï¼ˆæˆ–å¤šä¸ªç§å­å¹³å‡ï¼‰
                curve = results[0].reward_curve
                episodes = list(range(len(curve)))
                
                # å¹³æ»‘å¤„ç†
                window = min(10, len(curve) // 10)
                if window > 1:
                    smoothed = np.convolve(curve, np.ones(window)/window, mode='valid')
                    episodes_smooth = episodes[window-1:]
                else:
                    smoothed = curve
                    episodes_smooth = episodes
                
                ax2.plot(episodes_smooth, smoothed, label=method, 
                        color=colors.get(method, '#34495e'), linewidth=2.5, alpha=0.9)
        
        ax2.set_xlabel('è®­ç»ƒEpisode', fontsize=13, fontweight='bold')
        ax2.set_ylabel('ç´¯è®¡å¥–åŠ±', fontsize=13, fontweight='bold')
        ax2.set_title('äº”ç§æ–¹æ³•å­¦ä¹ æ›²çº¿å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax2.legend(fontsize=12, loc='lower right')
        ax2.grid(alpha=0.3, linestyle='--')
        
        plt.tight_layout()
        learning_curve_path = os.path.join(save_dir, 'learning_curves.png')
        plt.savefig(learning_curve_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"  ğŸ“ˆ å­¦ä¹ æ›²çº¿å¯¹æ¯”å›¾å·²ä¿å­˜: {learning_curve_path}")
        
        # ============ å›¾3ï¼šç»¼åˆæ€§èƒ½å¯¹æ¯” ============
        fig3 = plt.figure(figsize=(14, 10))
        gs = GridSpec(2, 2, figure=fig3, hspace=0.3, wspace=0.3)
        
        # å­å›¾1ï¼šè¾¾æ ‡ç‡
        ax31 = fig3.add_subplot(gs[0, 0])
        ax31.bar(method_names, accuracies, color=bar_colors, alpha=0.8, edgecolor='black')
        ax31.set_ylabel('è¾¾æ ‡ç‡ (%)', fontweight='bold')
        ax31.set_title('(a) Pmaxæ§åˆ¶è¾¾æ ‡ç‡', fontweight='bold')
        ax31.grid(axis='y', alpha=0.3)
        
        # å­å›¾2ï¼šå¹³å‡å¥–åŠ±
        ax32 = fig3.add_subplot(gs[0, 1])
        rewards = [avg_metrics[m]['reward'] for m in method_names]
        ax32.bar(method_names, rewards, color=bar_colors, alpha=0.8, edgecolor='black')
        ax32.set_ylabel('å¹³å‡å¥–åŠ±', fontweight='bold')
        ax32.set_title('(b) è®­ç»ƒç»ˆæœŸå¹³å‡å¥–åŠ±', fontweight='bold')
        ax32.grid(axis='y', alpha=0.3)
        
        # å­å›¾3ï¼šæ”¶æ•›é€Ÿåº¦
        ax33 = fig3.add_subplot(gs[1, 0])
        convergences = [avg_metrics[m]['convergence'] for m in method_names]
        ax33.bar(method_names, convergences, color=bar_colors, alpha=0.8, edgecolor='black')
        ax33.set_ylabel('æ”¶æ•›Episode', fontweight='bold')
        ax33.set_title('(c) æ”¶æ•›é€Ÿåº¦ (è¶Šå°è¶Šå¥½)', fontweight='bold')
        ax33.grid(axis='y', alpha=0.3)
        
        # å­å›¾4ï¼šè®­ç»ƒæ—¶é—´
        ax34 = fig3.add_subplot(gs[1, 1])
        times = [avg_metrics[m]['time'] for m in method_names]
        ax34.bar(method_names, times, color=bar_colors, alpha=0.8, edgecolor='black')
        ax34.set_ylabel('è®­ç»ƒæ—¶é—´ (ç§’)', fontweight='bold')
        ax34.set_title('(d) è®­ç»ƒè€—æ—¶', fontweight='bold')
        ax34.grid(axis='y', alpha=0.3)
        
        plt.suptitle('äº”ç§æ§åˆ¶æ–¹æ³•ç»¼åˆæ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold', y=0.995)
        
        plt.tight_layout()
        comprehensive_path = os.path.join(save_dir, 'five_method_comparison.png')
        plt.savefig(comprehensive_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"  ğŸ“Š ç»¼åˆå¯¹æ¯”å›¾å·²ä¿å­˜: {comprehensive_path}")
        
        # åŒæ—¶ä¿å­˜åˆ°visualization_outputç›®å½•
        vis_output_dir = 'visualization_output'
        os.makedirs(vis_output_dir, exist_ok=True)
        
        import shutil
        try:
            # å¤åˆ¶5æ–¹æ³•å¯¹æ¯”å›¾åˆ°visualization_output
            shutil.copy(accuracy_path, os.path.join(vis_output_dir, 'five_method_accuracy.png'))
            shutil.copy(learning_curve_path, os.path.join(vis_output_dir, 'five_method_learning_curves.png'))
            shutil.copy(comprehensive_path, os.path.join(vis_output_dir, 'five_method_comparison.png'))
            print(f"\nâœ… å¯è§†åŒ–å›¾è¡¨å·²åŒæ­¥åˆ°: {vis_output_dir}/")
        except Exception as e:
            print(f"âš ï¸ åŒæ­¥åˆ°visualization_outputå¤±è´¥: {e}")
        
        print(f"\nâœ… æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆåœ¨: {save_dir}")


# ============================================================
# å¯è§†åŒ–è¾…åŠ©å‡½æ•°
# ============================================================

def plot_training_progress(method_name: str, reward_curve: List[float], 
                           save_path: str = None):
    """
    ç»˜åˆ¶å•ä¸ªæ–¹æ³•çš„è®­ç»ƒè¿›åº¦å›¾
    
    Args:
        method_name: æ–¹æ³•åç§°
        reward_curve: å¥–åŠ±æ›²çº¿
        save_path: ä¿å­˜è·¯å¾„
    """
    if not MATPLOTLIB_AVAILABLE:
        return
    
    plt.figure(figsize=(10, 6))
    
    episodes = list(range(len(reward_curve)))
    
    # åŸå§‹æ›²çº¿ï¼ˆé€æ˜ï¼‰
    plt.plot(episodes, reward_curve, alpha=0.3, color='#3498db', linewidth=1)
    
    # å¹³æ»‘æ›²çº¿
    window = min(20, len(reward_curve) // 10)
    if window > 1:
        smoothed = np.convolve(reward_curve, np.ones(window)/window, mode='valid')
        episodes_smooth = episodes[window-1:]
        plt.plot(episodes_smooth, smoothed, color='#e74c3c', linewidth=2.5, label='å¹³æ»‘æ›²çº¿')
    
    plt.xlabel('è®­ç»ƒEpisode', fontsize=12, fontweight='bold')
    plt.ylabel('ç´¯è®¡å¥–åŠ±', fontsize=12, fontweight='bold')
    plt.title(f'{method_name} è®­ç»ƒè¿›åº¦', fontsize=14, fontweight='bold')
    plt.legend(fontsize=11)
    plt.grid(alpha=0.3, linestyle='--')
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"  ğŸ“ˆ è®­ç»ƒè¿›åº¦å›¾å·²ä¿å­˜: {save_path}")
    
    plt.close()


# ============================================================
# ä¸»å…¥å£
# ============================================================

def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("ğŸ”¬ æŸ´æ²¹æœºæ§åˆ¶æ–¹æ³•å¯¹æ¯”å®éªŒ")
    print("="*70)
    
    # åˆ›å»ºé…ç½®
    config = ExperimentConfig(
        n_episodes=400,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    print(f"\nå®éªŒé…ç½®:")
    print(f"  - Episodes: {config.n_episodes}")
    print(f"  - Batch Size: {config.batch_size}")
    print(f"  - Device: {config.device}")
    print(f"  - Seeds: {config.seeds}")
    
    # åˆ›å»ºå®éªŒ
    experiment = FiveMethodComparison(config)
    
    # å…ˆå¿«é€ŸéªŒè¯
    print("\n" + "-"*70)
    print("æ­¥éª¤1: å¿«é€ŸéªŒè¯ (å•ç§å­)")
    print("-"*70)
    
    validation_results = experiment.run_quick_validation()
    
    # è¯¢é—®æ˜¯å¦ç»§ç»­å®Œæ•´å®éªŒ
    print("\nå¿«é€ŸéªŒè¯å®Œæˆ! æ˜¯å¦ç»§ç»­å®Œæ•´å®éªŒ? (y/n)")
    # åœ¨è‡ªåŠ¨åŒ–åœºæ™¯ä¸‹é»˜è®¤ç»§ç»­
    # user_input = input().strip().lower()
    user_input = 'y'  # è‡ªåŠ¨ç»§ç»­
    
    if user_input == 'y':
        print("\n" + "-"*70)
        print("æ­¥éª¤2: å®Œæ•´å¯¹æ¯”å®éªŒ (5ä¸ªç§å­)")
        print("-"*70)
        
        comparison = experiment.run_full_comparison()
        
        print(f"\nâœ… å®éªŒå®Œæˆ! æ¨èä½¿ç”¨: {comparison.best_method}")
    else:
        print("\nå·²è·³è¿‡å®Œæ•´å®éªŒ")


if __name__ == "__main__":
    main()
