#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
2024-2025å¹´æœ€æ–°å¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç° (GPUåŠ é€Ÿ)
==========================================
åŒ…å«è¿‘ä¸¤å¹´é¡¶ä¼šé¡¶åˆŠçš„æœ€æ–°RLæ–¹æ³•:

1. Diffusion Policy (2024, RSS/CoRL) - æ‰©æ•£æ¨¡å‹ç”ŸæˆåŠ¨ä½œ
2. TD-MPC2 (2024, ICLR) - æ—¶åºå·®åˆ†æ¨¡å‹é¢„æµ‹æ§åˆ¶
3. Mamba Policy (2025) - åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹çš„ç­–ç•¥
4. DPMD (2025) - æ‰©æ•£ç­–ç•¥é•œåƒä¸‹é™

References:
- Diffusion Policy: Chi et al., "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion", RSS 2023 / CoRL 2024
- TD-MPC2: Hansen et al., "TD-MPC2: Scalable, Robust World Models for Continuous Control", ICLR 2024
- Mamba: Gu & Dao, "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", 2024

Author: CDC Project
Date: 2026-01-21
"""

import numpy as np
from collections import deque
import random
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import warnings
import math
import time

# å°è¯•å¯¼å…¥æ·±åº¦å­¦ä¹ åº“
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    from torch.distributions import Normal, Categorical
    TORCH_AVAILABLE = True
    
    # GPUæ£€æµ‹ä¸è‡ªåŠ¨é…ç½®
    def get_device(prefer_gpu: bool = True) -> torch.device:
        """æ™ºèƒ½è®¾å¤‡é€‰æ‹©"""
        if prefer_gpu and torch.cuda.is_available():
            device = torch.device('cuda')
            print(f"[AdvancedRL] ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
            print(f"[AdvancedRL] GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            return device
        else:
            print("[AdvancedRL] ä½¿ç”¨CPU")
            return torch.device('cpu')
    
    def auto_batch_size(device: torch.device, base_batch: int = 256) -> int:
        """æ ¹æ®GPUæ˜¾å­˜è‡ªåŠ¨è°ƒæ•´batch_size"""
        if device.type == 'cuda':
            gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
            if gpu_mem >= 8:
                return min(512, base_batch * 2)
            elif gpu_mem >= 4:
                return base_batch
            else:
                return max(64, base_batch // 2)
        return base_batch
    
    DEFAULT_DEVICE = get_device()
    
except ImportError:
    TORCH_AVAILABLE = False
    DEFAULT_DEVICE = None
    warnings.warn("PyTorch not available. Advanced RL algorithms will not work.")


# å¯¼å…¥åŸºç±»
try:
    from .rl_algorithms import BaseRLAlgorithm, ReplayBuffer, Experience
except ImportError:
    from rl_algorithms import BaseRLAlgorithm, ReplayBuffer, Experience


if TORCH_AVAILABLE:
    
    # ============================================================
    # GPUå·¥å…·å‡½æ•°
    # ============================================================
    
    def to_device(tensor_or_array, device):
        """å°†numpyæ•°ç»„æˆ–tensorç§»åˆ°æŒ‡å®šè®¾å¤‡"""
        if isinstance(tensor_or_array, np.ndarray):
            return torch.FloatTensor(tensor_or_array).to(device)
        elif isinstance(tensor_or_array, torch.Tensor):
            return tensor_or_array.to(device)
        return tensor_or_array
    
    
    # ============================================================
    # 1. Diffusion Policy (RSS 2023 / CoRL 2024)
    # ============================================================
    
    class SinusoidalPosEmb(nn.Module):
        """æ­£å¼¦ä½ç½®ç¼–ç ï¼ˆç”¨äºæ‰©æ•£æ—¶é—´æ­¥ï¼‰"""
        def __init__(self, dim):
            super().__init__()
            self.dim = dim
        
        def forward(self, t):
            device = t.device
            half_dim = self.dim // 2
            emb = math.log(10000) / (half_dim - 1)
            emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
            emb = t[:, None].float() * emb[None, :]
            emb = torch.cat([emb.sin(), emb.cos()], dim=-1)
            return emb
    
    
    class DiffusionMLP(nn.Module):
        """æ‰©æ•£æ¨¡å‹çš„MLPç½‘ç»œ"""
        def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
            super().__init__()
            
            self.time_mlp = nn.Sequential(
                SinusoidalPosEmb(hidden_dim),
                nn.Linear(hidden_dim, hidden_dim),
                nn.Mish(),
                nn.Linear(hidden_dim, hidden_dim)
            )
            
            self.state_encoder = nn.Sequential(
                nn.Linear(state_dim, hidden_dim),
                nn.Mish(),
                nn.Linear(hidden_dim, hidden_dim)
            )
            
            self.noise_pred = nn.Sequential(
                nn.Linear(hidden_dim + hidden_dim + action_dim, hidden_dim),
                nn.Mish(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.Mish(),
                nn.Linear(hidden_dim, action_dim)
            )
        
        def forward(self, x, t, state):
            t_emb = self.time_mlp(t)
            s_emb = self.state_encoder(state)
            combined = torch.cat([x, t_emb, s_emb], dim=-1)
            return self.noise_pred(combined)
    
    
    class DiffusionPolicy(BaseRLAlgorithm):
        """
        Diffusion Policy (Chi et al., RSS 2023 / CoRL 2024)
        
        ä½¿ç”¨DDPMæ‰©æ•£æ¨¡å‹ç”ŸæˆåŠ¨ä½œï¼Œé€šè¿‡å»å™ªè¿‡ç¨‹ä»å™ªå£°ä¸­æ¢å¤æœ€ä¼˜åŠ¨ä½œã€‚
        GPUåŠ é€Ÿç‰ˆæœ¬ã€‚
        """
        
        def __init__(self, state_dim: int, action_dim: int, config: Dict = None):
            super().__init__(state_dim, action_dim, config)
            
            # GPUè®¾ç½®
            self.device = torch.device(config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))
            self.lr = config.get('lr', 1e-4)
            self.gamma = config.get('gamma', 0.99)
            self.batch_size = auto_batch_size(self.device, config.get('batch_size', 128))
            
            # æ‰©æ•£å‚æ•° (ä¼˜åŒ–ï¼šå‡å°‘æ­¥æ•°åŠ å¿«è®­ç»ƒ)
            self.n_diffusion_steps = config.get('n_diffusion_steps', 5)  # åŸ20
            self.beta_start = 1e-4
            self.beta_end = 0.02
            
            # Beta schedule (é¢„è®¡ç®—ï¼Œæ”¾GPU)
            self.betas = torch.linspace(self.beta_start, self.beta_end, self.n_diffusion_steps).to(self.device)
            self.alphas = 1. - self.betas
            self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)
            self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)
            self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)
            
            # ç½‘ç»œ -> GPU
            hidden_dim = config.get('hidden_dim', 256)
            self.model = DiffusionMLP(state_dim, action_dim, hidden_dim).to(self.device)
            self.optimizer = optim.AdamW(self.model.parameters(), lr=self.lr)
            
            # ç»éªŒå›æ”¾
            self.buffer = ReplayBuffer(config.get('buffer_size', 100000))
            
            # æ¢ç´¢
            self.epsilon = config.get('epsilon', 0.3)
            self.epsilon_min = config.get('epsilon_min', 0.05)
            self.epsilon_decay = config.get('epsilon_decay', 0.995)
            
            print(f"[DiffusionPolicy] åˆå§‹åŒ–å®Œæˆ | Device: {self.device} | Batch: {self.batch_size}")
        
        def _q_sample(self, x_start, t, noise=None):
            """å‰å‘æ‰©æ•£ - æ·»åŠ å™ªå£°"""
            if noise is None:
                noise = torch.randn_like(x_start)
            
            sqrt_alpha = self.sqrt_alphas_cumprod[t].view(-1, 1)
            sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1)
            
            return sqrt_alpha * x_start + sqrt_one_minus_alpha * noise, noise
        
        def _p_sample(self, x, t, state):
            """åå‘å»å™ªä¸€æ­¥"""
            pred_noise = self.model(x, t, state)
            
            alpha = self.alphas[t].view(-1, 1)
            beta = self.betas[t].view(-1, 1)
            
            mean = (1 / torch.sqrt(alpha)) * (
                x - (beta / self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1)) * pred_noise
            )
            
            if t[0] > 0:
                noise = torch.randn_like(x)
                std = torch.sqrt(beta)
                return mean + std * noise
            else:
                return mean
        
        def _sample_action(self, state: torch.Tensor) -> torch.Tensor:
            """ä»æ‰©æ•£æ¨¡å‹é‡‡æ ·åŠ¨ä½œ"""
            batch_size = state.shape[0]
            x = torch.randn(batch_size, self.action_dim, device=self.device)
            
            for t in reversed(range(self.n_diffusion_steps)):
                t_batch = torch.full((batch_size,), t, dtype=torch.long, device=self.device)
                x = self._p_sample(x, t_batch, state)
            
            return x
        
        def select_action(self, state: np.ndarray, explore: bool = True) -> int:
            if explore and random.random() < self.epsilon:
                return random.randint(0, self.action_dim - 1)
            
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                action_continuous = self._sample_action(state_tensor)
                action_idx = torch.argmax(action_continuous, dim=-1).item()
                return int(np.clip(action_idx, 0, self.action_dim - 1))
        
        def update(self, batch: List) -> Dict[str, float]:
            if len(batch) < self.batch_size:
                return {}
            
            # æ•°æ®ç§»åˆ°GPU
            states = torch.FloatTensor(np.array([e.state for e in batch])).to(self.device)
            actions = torch.LongTensor([e.action for e in batch]).to(self.device)
            
            action_onehot = F.one_hot(actions, self.action_dim).float()
            t = torch.randint(0, self.n_diffusion_steps, (len(batch),), device=self.device)
            
            noisy_actions, noise = self._q_sample(action_onehot, t)
            pred_noise = self.model(noisy_actions, t, states)
            
            loss = F.mse_loss(pred_noise, noise)
            
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
            self.training_step += 1
            
            return {'loss': loss.item(), 'epsilon': self.epsilon}
        
        def save(self, path: str):
            torch.save({
                'model': self.model.state_dict(),
                'optimizer': self.optimizer.state_dict(),
                'epsilon': self.epsilon,
                'training_step': self.training_step
            }, path)
        
        def load(self, path: str):
            checkpoint = torch.load(path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model'])
            self.optimizer.load_state_dict(checkpoint['optimizer'])
            self.epsilon = checkpoint['epsilon']
            self.training_step = checkpoint['training_step']
    
    
    # ============================================================
    # 2. TD-MPC2 (ICLR 2024)
    # ============================================================
    
    class WorldModel(nn.Module):
        """TD-MPC2çš„ä¸–ç•Œæ¨¡å‹"""
        def __init__(self, state_dim: int, action_dim: int, latent_dim: int = 256):
            super().__init__()
            
            # çŠ¶æ€ç¼–ç å™¨
            self.encoder = nn.Sequential(
                nn.Linear(state_dim, 512),
                nn.LayerNorm(512),
                nn.Mish(),
                nn.Linear(512, latent_dim)
            )
            
            # åŠ¨åŠ›å­¦æ¨¡å‹
            self.dynamics = nn.Sequential(
                nn.Linear(latent_dim + action_dim, 512),
                nn.LayerNorm(512),
                nn.Mish(),
                nn.Linear(512, latent_dim)
            )
            
            # å¥–åŠ±é¢„æµ‹
            self.reward_pred = nn.Sequential(
                nn.Linear(latent_dim, 256),
                nn.Mish(),
                nn.Linear(256, 1)
            )
            
            # Qç½‘ç»œ
            self.q_network = nn.Sequential(
                nn.Linear(latent_dim, 512),
                nn.Mish(),
                nn.Linear(512, action_dim)
            )
        
        def encode(self, state):
            return self.encoder(state)
        
        def predict_next(self, latent, action_onehot):
            x = torch.cat([latent, action_onehot], dim=-1)
            return self.dynamics(x)
        
        def predict_reward(self, latent):
            return self.reward_pred(latent)
        
        def get_q_values(self, latent):
            return self.q_network(latent)
    
    
    class TDMPC2(BaseRLAlgorithm):
        """
        TD-MPC2 (Hansen et al., ICLR 2024)
        
        ç»“åˆæ—¶åºå·®åˆ†å­¦ä¹ å’Œæ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼Œä½¿ç”¨å­¦ä¹ çš„ä¸–ç•Œæ¨¡å‹è¿›è¡Œè§„åˆ’ã€‚
        GPUåŠ é€Ÿç‰ˆæœ¬ï¼Œä½¿ç”¨CEMä¼˜åŒ–å™¨è¿›è¡Œåœ¨çº¿è§„åˆ’ã€‚
        """
        
        def __init__(self, state_dim: int, action_dim: int, config: Dict = None):
            super().__init__(state_dim, action_dim, config)
            
            # GPUè®¾ç½®
            self.device = torch.device(config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))
            self.lr = config.get('lr', 3e-4)
            self.gamma = config.get('gamma', 0.99)
            self.tau = config.get('tau', 0.005)
            self.batch_size = auto_batch_size(self.device, config.get('batch_size', 128))
            
            # MPCè§„åˆ’å‚æ•° (ä¼˜åŒ–ï¼šå¤§å¹…å‡å°‘è®¡ç®—é‡)
            self.horizon = config.get('horizon', 2)  # åŸ3
            self.n_samples = config.get('n_samples', 16)  # åŸ32
            self.n_elites = config.get('n_elites', 4)  # åŸ6
            self.n_iterations = config.get('n_iterations', 1)  # åŸ2
            
            latent_dim = config.get('latent_dim', 256)
            
            # ä¸–ç•Œæ¨¡å‹ -> GPU
            self.world_model = WorldModel(state_dim, action_dim, latent_dim).to(self.device)
            self.target_world_model = WorldModel(state_dim, action_dim, latent_dim).to(self.device)
            self.target_world_model.load_state_dict(self.world_model.state_dict())
            
            self.optimizer = optim.Adam(self.world_model.parameters(), lr=self.lr)
            self.buffer = ReplayBuffer(config.get('buffer_size', 100000))
            
            # æ¢ç´¢
            self.epsilon = config.get('epsilon', 0.3)
            self.epsilon_min = config.get('epsilon_min', 0.05)
            self.epsilon_decay = config.get('epsilon_decay', 0.995)
            
            print(f"[TD-MPC2] åˆå§‹åŒ–å®Œæˆ | Device: {self.device} | Horizon: {self.horizon}")
        
        @torch.no_grad()
        def _plan(self, state: torch.Tensor) -> int:
            """CEMè§„åˆ’ - äº¤å‰ç†µæ–¹æ³•ä¼˜åŒ–åŠ¨ä½œåºåˆ—"""
            latent = self.world_model.encode(state)  # (1, latent_dim)
            
            # åˆå§‹åŒ–åŠ¨ä½œåˆ†å¸ƒ
            mean = torch.zeros(self.horizon, self.action_dim, device=self.device)
            std = torch.ones(self.horizon, self.action_dim, device=self.device) * 2.0
            
            for _ in range(self.n_iterations):
                # é‡‡æ ·åŠ¨ä½œåºåˆ—
                samples = mean.unsqueeze(0) + std.unsqueeze(0) * torch.randn(
                    self.n_samples, self.horizon, self.action_dim, device=self.device
                )
                
                # è¯„ä¼°æ¯ä¸ªåºåˆ—çš„å›æŠ¥
                returns = torch.zeros(self.n_samples, device=self.device)
                
                for i in range(self.n_samples):
                    z = latent.clone()  # (1, latent_dim)
                    total_return = 0
                    discount = 1.0
                    
                    for t in range(self.horizon):
                        action_probs = F.softmax(samples[i, t], dim=-1)  # (action_dim,)
                        reward = self.world_model.predict_reward(z)  # z: (1, latent_dim)
                        # predict_nextéœ€è¦ (batch, latent_dim) å’Œ (batch, action_dim)
                        z = self.world_model.predict_next(z, action_probs.unsqueeze(0))  # (1, latent_dim)
                        total_return += discount * reward.item()
                        discount *= self.gamma
                    
                    # ç»ˆç«¯ä»·å€¼ä¼°è®¡
                    q_values = self.world_model.get_q_values(z)  # z already (1, latent_dim)
                    returns[i] = total_return + discount * q_values.max().item()
                
                # é€‰æ‹©ç²¾è‹±æ ·æœ¬æ›´æ–°åˆ†å¸ƒ
                elite_idx = returns.argsort(descending=True)[:self.n_elites]
                elite_samples = samples[elite_idx]
                mean = elite_samples.mean(dim=0)
                std = elite_samples.std(dim=0) + 0.1
            
            # è¿”å›ç¬¬ä¸€æ­¥çš„æœ€ä¼˜åŠ¨ä½œ
            return mean[0].argmax().item()
        
        def select_action(self, state: np.ndarray, explore: bool = True) -> int:
            if explore and random.random() < self.epsilon:
                return random.randint(0, self.action_dim - 1)
            
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            return self._plan(state_tensor)
        
        def update(self, batch: List) -> Dict[str, float]:
            if len(batch) < self.batch_size:
                return {}
            
            # æ•°æ®ç§»åˆ°GPU
            states = torch.FloatTensor(np.array([e.state for e in batch])).to(self.device)
            actions = torch.LongTensor([e.action for e in batch]).to(self.device)
            rewards = torch.FloatTensor([e.reward for e in batch]).to(self.device)
            next_states = torch.FloatTensor(np.array([e.next_state for e in batch])).to(self.device)
            dones = torch.FloatTensor([e.done for e in batch]).to(self.device)
            
            action_onehot = F.one_hot(actions, self.action_dim).float()
            
            # ç¼–ç 
            latent = self.world_model.encode(states)
            next_latent = self.world_model.encode(next_states)
            
            # åŠ¨åŠ›å­¦æŸå¤±
            pred_next_latent = self.world_model.predict_next(latent, action_onehot)
            dynamics_loss = F.mse_loss(pred_next_latent, next_latent.detach())
            
            # å¥–åŠ±é¢„æµ‹æŸå¤±
            pred_reward = self.world_model.predict_reward(latent).squeeze()
            reward_loss = F.mse_loss(pred_reward, rewards)
            
            # TDæŸå¤±
            q_values = self.world_model.get_q_values(latent)
            current_q = q_values.gather(1, actions.unsqueeze(1)).squeeze()
            
            with torch.no_grad():
                next_q = self.target_world_model.get_q_values(next_latent)
                target_q = rewards + self.gamma * next_q.max(dim=1)[0] * (1 - dones)
            
            q_loss = F.mse_loss(current_q, target_q)
            
            # æ€»æŸå¤±
            loss = dynamics_loss + reward_loss + q_loss
            
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.world_model.parameters(), 1.0)
            self.optimizer.step()
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            for param, target_param in zip(self.world_model.parameters(), 
                                          self.target_world_model.parameters()):
                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
            
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
            self.training_step += 1
            
            return {
                'loss': loss.item(),
                'dynamics_loss': dynamics_loss.item(),
                'reward_loss': reward_loss.item(),
                'q_loss': q_loss.item()
            }
        
        def save(self, path: str):
            torch.save({
                'world_model': self.world_model.state_dict(),
                'target_world_model': self.target_world_model.state_dict(),
                'optimizer': self.optimizer.state_dict(),
                'epsilon': self.epsilon,
                'training_step': self.training_step
            }, path)
        
        def load(self, path: str):
            checkpoint = torch.load(path, map_location=self.device)
            self.world_model.load_state_dict(checkpoint['world_model'])
            self.target_world_model.load_state_dict(checkpoint['target_world_model'])
            self.optimizer.load_state_dict(checkpoint['optimizer'])
            self.epsilon = checkpoint['epsilon']
            self.training_step = checkpoint['training_step']
    
    
    # ============================================================
    # 3. Mamba Policy (2025)
    # ============================================================
    
    class SelectiveSSM(nn.Module):
        """
        é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ (Mambaæ ¸å¿ƒ)
        çº¿æ€§æ—¶é—´å¤æ‚åº¦O(L)ï¼Œæ›¿ä»£Transformerçš„O(LÂ²)
        """
        def __init__(self, d_model: int, d_state: int = 16, d_conv: int = 4):
            super().__init__()
            
            self.d_model = d_model
            self.d_state = d_state
            
            # è¾“å…¥æŠ•å½±
            self.in_proj = nn.Linear(d_model, d_model * 2)
            
            # 1Då·ç§¯ï¼ˆå› æœï¼‰
            self.conv1d = nn.Conv1d(d_model, d_model, kernel_size=d_conv, 
                                   padding=d_conv - 1, groups=d_model)
            
            # SSMå‚æ•°æŠ•å½±
            self.x_proj = nn.Linear(d_model, d_state * 2 + 1, bias=False)
            
            # çŠ¶æ€çŸ©é˜µAçš„å¯¹æ•°ï¼ˆå¯å­¦ä¹ ï¼‰
            A = torch.arange(1, d_state + 1, dtype=torch.float32)
            self.A_log = nn.Parameter(torch.log(A.repeat(d_model, 1)))
            
            # è·³è·ƒè¿æ¥
            self.D = nn.Parameter(torch.ones(d_model))
            
            # è¾“å‡ºæŠ•å½±
            self.out_proj = nn.Linear(d_model, d_model)
        
        def forward(self, x):
            B, L, D = x.shape
            
            # åŒåˆ†æ”¯ï¼šxå’Œz
            xz = self.in_proj(x)
            x, z = xz.chunk(2, dim=-1)
            
            # å·ç§¯
            x = x.transpose(1, 2)
            x = self.conv1d(x)[:, :, :L]
            x = x.transpose(1, 2)
            x = F.silu(x)
            
            # SSM
            y = self._ssm(x)
            
            # é—¨æ§
            y = y * F.silu(z)
            
            return self.out_proj(y)
        
        def _ssm(self, x):
            """é€‰æ‹©æ€§æ‰«æ"""
            B, L, D = x.shape
            
            # æŠ•å½±å¾—åˆ°delta, B, C
            x_proj = self.x_proj(x)
            delta, B_proj, C = x_proj.split([1, self.d_state, self.d_state], dim=-1)
            delta = F.softplus(delta)
            
            # ç¦»æ•£åŒ–A
            A = -torch.exp(self.A_log)
            
            # å¾ªç¯æ‰«æï¼ˆç®€åŒ–ç‰ˆï¼Œå®Œæ•´ç‰ˆåº”ä½¿ç”¨å¹¶è¡Œæ‰«æï¼‰
            h = torch.zeros(B, D, self.d_state, device=x.device)
            ys = []
            
            for i in range(L):
                dt = delta[:, i, 0].unsqueeze(-1).unsqueeze(-1)
                dA = torch.exp(A.unsqueeze(0) * dt)
                dB = B_proj[:, i].unsqueeze(1).expand(-1, D, -1)
                
                h = dA * h + dB * x[:, i].unsqueeze(-1)
                
                C_t = C[:, i].unsqueeze(1).expand(-1, D, -1)
                y = (h * C_t).sum(dim=-1)
                ys.append(y)
            
            y = torch.stack(ys, dim=1)
            
            # è·³è·ƒè¿æ¥
            y = y + x * self.D.unsqueeze(0).unsqueeze(0)
            
            return y
    
    
    class MambaBlock(nn.Module):
        """Mambaå—ï¼šLayerNorm + SSM + æ®‹å·®"""
        def __init__(self, d_model: int):
            super().__init__()
            self.norm = nn.LayerNorm(d_model)
            self.mamba = SelectiveSSM(d_model)
        
        def forward(self, x):
            return x + self.mamba(self.norm(x))
    
    
    class MambaPolicy(BaseRLAlgorithm):
        """
        Mamba Policy (2025)
        
        åŸºäºMambaé€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚
        O(L)å¤æ‚åº¦æ›¿ä»£O(LÂ²)çš„Transformerï¼Œé€‚åˆå®æ—¶æ§åˆ¶ã€‚
        GPUåŠ é€Ÿç‰ˆæœ¬ã€‚
        """
        
        def __init__(self, state_dim: int, action_dim: int, config: Dict = None):
            super().__init__(state_dim, action_dim, config)
            
            # GPUè®¾ç½®
            self.device = torch.device(config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))
            self.lr = config.get('lr', 1e-4)
            self.gamma = config.get('gamma', 0.99)
            self.batch_size = auto_batch_size(self.device, config.get('batch_size', 64))
            self.context_length = config.get('context_length', 20)
            
            # Mambaå‚æ•°
            d_model = config.get('d_model', 128)
            n_layers = config.get('n_layers', 4)
            
            # åµŒå…¥å±‚ -> GPU
            self.state_embed = nn.Linear(state_dim, d_model).to(self.device)
            self.action_embed = nn.Embedding(action_dim, d_model).to(self.device)
            self.return_embed = nn.Linear(1, d_model).to(self.device)
            
            # Mambaå±‚ -> GPU
            self.mamba_layers = nn.ModuleList([
                MambaBlock(d_model) for _ in range(n_layers)
            ]).to(self.device)
            
            # è¾“å‡ºå¤´ -> GPU
            self.action_head = nn.Linear(d_model, action_dim).to(self.device)
            self.value_head = nn.Linear(d_model, 1).to(self.device)
            self.ln = nn.LayerNorm(d_model).to(self.device)
            
            # æ”¶é›†æ‰€æœ‰å‚æ•°
            all_params = (
                list(self.state_embed.parameters()) +
                list(self.action_embed.parameters()) +
                list(self.return_embed.parameters()) +
                list(self.mamba_layers.parameters()) +
                list(self.action_head.parameters()) +
                list(self.value_head.parameters()) +
                list(self.ln.parameters())
            )
            self.mamba_optimizer = optim.Adam(all_params, lr=self.lr)
            
            # å†å²ç¼“å­˜
            self.state_history = deque(maxlen=self.context_length)
            self.action_history = deque(maxlen=self.context_length)
            self.return_history = deque(maxlen=self.context_length)
            
            # è½¨è¿¹å­˜å‚¨ï¼ˆç”¨äºåºåˆ—è®­ç»ƒï¼‰
            self.trajectories = []
            self.target_return = config.get('target_return', 100)
            
            # Qç½‘ç»œï¼ˆç”¨äºå¿«é€Ÿåœ¨çº¿å†³ç­–ï¼‰ -> GPU
            self.q_network = nn.Sequential(
                nn.Linear(state_dim, 256),
                nn.ReLU(),
                nn.Linear(256, 256),
                nn.ReLU(),
                nn.Linear(256, action_dim)
            ).to(self.device)
            self.q_optimizer = optim.Adam(self.q_network.parameters(), lr=self.lr)
            
            # æ¢ç´¢
            self.epsilon = config.get('epsilon', 0.3)
            self.epsilon_min = config.get('epsilon_min', 0.05)
            self.epsilon_decay = config.get('epsilon_decay', 0.995)
            
            self.buffer = ReplayBuffer(config.get('buffer_size', 100000))
            
            print(f"[MambaPolicy] åˆå§‹åŒ–å®Œæˆ | Device: {self.device} | Layers: {n_layers}")
        
        def forward(self, states, actions=None, returns=None):
            """åºåˆ—å‰å‘ä¼ æ’­"""
            B, L, _ = states.shape
            
            # åµŒå…¥
            s_emb = self.state_embed(states)
            
            if actions is not None:
                a_emb = self.action_embed(actions)
            else:
                a_emb = torch.zeros_like(s_emb)
            
            if returns is not None:
                r_emb = self.return_embed(returns.unsqueeze(-1))
            else:
                r_emb = torch.zeros_like(s_emb)
            
            # èåˆ
            x = s_emb + a_emb + r_emb
            
            # Mambaå±‚
            for layer in self.mamba_layers:
                x = layer(x)
            
            x = self.ln(x)
            
            # è¾“å‡º
            action_logits = self.action_head(x)
            values = self.value_head(x)
            
            return action_logits, values
        
        def select_action(self, state: np.ndarray, explore: bool = True) -> int:
            if explore and random.random() < self.epsilon:
                action = random.randint(0, self.action_dim - 1)
                self.state_history.append(state)
                self.action_history.append(action)
                return action
            
            self.state_history.append(state)
            
            with torch.no_grad():
                # ä½¿ç”¨Qç½‘ç»œå¿«é€Ÿå†³ç­–
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                q_values = self.q_network(state_tensor)
                
                if explore:
                    probs = F.softmax(q_values / 0.5, dim=-1)
                    action = torch.multinomial(probs, 1).item()
                else:
                    action = q_values.argmax().item()
            
            self.action_history.append(action)
            return action
        
        def store_trajectory(self, states, actions, rewards):
            """å­˜å‚¨å®Œæ•´è½¨è¿¹ç”¨äºåºåˆ—è®­ç»ƒ"""
            returns = []
            R = 0
            for r in reversed(rewards):
                R = r + self.gamma * R
                returns.insert(0, R)
            
            self.trajectories.append({
                'states': np.array(states),
                'actions': np.array(actions),
                'returns': np.array(returns)
            })
            
            # é™åˆ¶å­˜å‚¨çš„è½¨è¿¹æ•°é‡
            if len(self.trajectories) > 100:
                self.trajectories = self.trajectories[-100:]
        
        def update(self, batch: List = None) -> Dict[str, float]:
            q_loss = torch.tensor(0.0, device=self.device)
            mamba_loss = torch.tensor(0.0, device=self.device)
            
            # Qç½‘ç»œåœ¨çº¿æ›´æ–°
            if batch and len(batch) >= self.batch_size:
                states = torch.FloatTensor(np.array([e.state for e in batch])).to(self.device)
                actions = torch.LongTensor([e.action for e in batch]).to(self.device)
                rewards = torch.FloatTensor([e.reward for e in batch]).to(self.device)
                next_states = torch.FloatTensor(np.array([e.next_state for e in batch])).to(self.device)
                dones = torch.FloatTensor([e.done for e in batch]).to(self.device)
                
                q_values = self.q_network(states)
                current_q = q_values.gather(1, actions.unsqueeze(1)).squeeze()
                
                with torch.no_grad():
                    next_q = self.q_network(next_states).max(dim=1)[0]
                    target_q = rewards + self.gamma * next_q * (1 - dones)
                
                q_loss = F.mse_loss(current_q, target_q)
                
                self.q_optimizer.zero_grad()
                q_loss.backward()
                self.q_optimizer.step()
            
            # Mambaåºåˆ—æ¨¡å‹ç¦»çº¿æ›´æ–°
            if len(self.trajectories) >= 3:
                for _ in range(3):
                    traj = random.choice(self.trajectories)
                    traj_len = len(traj['states'])
                    
                    if traj_len < self.context_length:
                        continue
                    
                    start = random.randint(0, traj_len - self.context_length)
                    end = start + self.context_length
                    
                    states = torch.FloatTensor(traj['states'][start:end]).unsqueeze(0).to(self.device)
                    actions = torch.LongTensor(traj['actions'][start:end]).unsqueeze(0).to(self.device)
                    returns = torch.FloatTensor(traj['returns'][start:end]).unsqueeze(0).to(self.device)
                    
                    # é¢„æµ‹ä¸‹ä¸€æ­¥åŠ¨ä½œï¼šè¾“å…¥å‰L-1ä¸ªçŠ¶æ€å’ŒåŠ¨ä½œï¼Œé¢„æµ‹L-1ä¸ªåŠ¨ä½œ
                    # states[:, :-1]: (1, L-1, state_dim)
                    # actions[:, :-1]: (1, L-1) 
                    # returns[:, :-1]: (1, L-1)
                    action_logits, values = self.forward(
                        states[:, :-1], 
                        actions[:, :-1], 
                        returns[:, :-1]
                    )
                    
                    # åŠ¨ä½œé¢„æµ‹æŸå¤±ï¼šé¢„æµ‹ä¸‹ä¸€æ­¥åŠ¨ä½œ
                    action_loss = F.cross_entropy(
                        action_logits.reshape(-1, self.action_dim),
                        actions[:, 1:].reshape(-1)
                    )
                    
                    # ä»·å€¼é¢„æµ‹æŸå¤±
                    value_loss = F.mse_loss(values.squeeze(-1), returns[:, :-1])
                    
                    loss = action_loss + 0.5 * value_loss
                    
                    self.mamba_optimizer.zero_grad()
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.mamba_layers.parameters(), 1.0)
                    self.mamba_optimizer.step()
                    
                    mamba_loss = loss
            
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
            self.training_step += 1
            
            return {'q_loss': q_loss.item(), 'mamba_loss': mamba_loss.item()}
        
        def reset_history(self):
            """é‡ç½®å†å²ç¼“å­˜ï¼ˆæ¯ä¸ªepisodeå¼€å§‹æ—¶è°ƒç”¨ï¼‰"""
            self.state_history.clear()
            self.action_history.clear()
            self.return_history.clear()
        
        def save(self, path: str):
            torch.save({
                'state_embed': self.state_embed.state_dict(),
                'action_embed': self.action_embed.state_dict(),
                'return_embed': self.return_embed.state_dict(),
                'mamba_layers': self.mamba_layers.state_dict(),
                'action_head': self.action_head.state_dict(),
                'value_head': self.value_head.state_dict(),
                'ln': self.ln.state_dict(),
                'q_network': self.q_network.state_dict(),
                'epsilon': self.epsilon,
                'training_step': self.training_step
            }, path)
        
        def load(self, path: str):
            checkpoint = torch.load(path, map_location=self.device)
            self.state_embed.load_state_dict(checkpoint['state_embed'])
            self.action_embed.load_state_dict(checkpoint['action_embed'])
            self.return_embed.load_state_dict(checkpoint['return_embed'])
            self.mamba_layers.load_state_dict(checkpoint['mamba_layers'])
            self.action_head.load_state_dict(checkpoint['action_head'])
            self.value_head.load_state_dict(checkpoint['value_head'])
            self.ln.load_state_dict(checkpoint['ln'])
            self.q_network.load_state_dict(checkpoint['q_network'])
            self.epsilon = checkpoint['epsilon']
            self.training_step = checkpoint['training_step']
    
    
    # ============================================================
    # 4. DPMD - Diffusion Policy Mirror Descent (2025)
    # ============================================================
    
    class DPMD(BaseRLAlgorithm):
        """
        Diffusion Policy with Mirror Descent (DPMD, 2025)
        
        ç»“åˆæ‰©æ•£ç­–ç•¥å’Œé•œåƒä¸‹é™ä¼˜åŒ–ï¼š
        - æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šæ¨¡æ€åŠ¨ä½œåˆ†å¸ƒ
        - é•œåƒä¸‹é™ç¡®ä¿ç­–ç•¥æ›´æ–°ç¨³å®š
        - KLçº¦æŸé˜²æ­¢ç­–ç•¥å´©æºƒ
        
        GPUåŠ é€Ÿç‰ˆæœ¬ã€‚
        """
        
        def __init__(self, state_dim: int, action_dim: int, config: Dict = None):
            super().__init__(state_dim, action_dim, config)
            
            # GPUè®¾ç½®
            self.device = torch.device(config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))
            self.lr = config.get('lr', 1e-4)
            self.gamma = config.get('gamma', 0.99)
            self.batch_size = auto_batch_size(self.device, config.get('batch_size', 128))
            
            # æ‰©æ•£å‚æ•° (ä¼˜åŒ–ï¼šå‡å°‘æ­¥æ•°åŠ å¿«è®­ç»ƒ)
            self.n_diffusion_steps = config.get('n_diffusion_steps', 5)  # åŸ10
            self.kl_coef = config.get('kl_coef', 0.1)  # é•œåƒä¸‹é™KLç³»æ•°
            
            hidden_dim = config.get('hidden_dim', 256)
            
            # æ‰©æ•£ç­–ç•¥ç½‘ç»œ -> GPU
            self.policy_net = nn.Sequential(
                nn.Linear(state_dim + action_dim + 1, hidden_dim),  # +1 for time embedding
                nn.Mish(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.Mish(),
                nn.Linear(hidden_dim, action_dim)
            ).to(self.device)
            
            # å‚è€ƒç­–ç•¥ï¼ˆç”¨äºKLçº¦æŸï¼‰-> ä½¿ç”¨ç›¸åŒç»“æ„ä»¥ä¾¿å¤åˆ¶æƒé‡
            self.ref_policy = nn.Sequential(
                nn.Linear(state_dim + action_dim + 1, hidden_dim),
                nn.Mish(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.Mish(),
                nn.Linear(hidden_dim, action_dim)
            ).to(self.device)
            # åˆå§‹åŒ–æ—¶å¤åˆ¶ç­–ç•¥ç½‘ç»œæƒé‡
            self.ref_policy.load_state_dict(self.policy_net.state_dict())
            
            # Qç½‘ç»œ -> GPU
            self.q_network = nn.Sequential(
                nn.Linear(state_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, action_dim)
            ).to(self.device)
            
            # ç›®æ ‡Qç½‘ç»œ -> GPU
            self.target_q = nn.Sequential(
                nn.Linear(state_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, action_dim)
            ).to(self.device)
            self.target_q.load_state_dict(self.q_network.state_dict())
            
            # æ‰©æ•£schedule -> GPU
            self.betas = torch.linspace(1e-4, 0.02, self.n_diffusion_steps).to(self.device)
            self.alphas = 1. - self.betas
            self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)
            
            # ä¼˜åŒ–å™¨
            self.optimizer = optim.Adam(
                list(self.policy_net.parameters()) + list(self.q_network.parameters()),
                lr=self.lr
            )
            
            self.buffer = ReplayBuffer(config.get('buffer_size', 100000))
            
            # æ¢ç´¢
            self.epsilon = config.get('epsilon', 0.3)
            self.epsilon_min = config.get('epsilon_min', 0.05)
            self.epsilon_decay = config.get('epsilon_decay', 0.995)
            self.tau = config.get('tau', 0.005)
            
            print(f"[DPMD] åˆå§‹åŒ–å®Œæˆ | Device: {self.device} | KL_coef: {self.kl_coef}")
        
        def _sample_action_dist(self, state: torch.Tensor) -> torch.Tensor:
            """æ‰©æ•£é‡‡æ ·åŠ¨ä½œåˆ†å¸ƒ"""
            B = state.shape[0]
            x = torch.randn(B, self.action_dim, device=self.device)
            
            for t in reversed(range(self.n_diffusion_steps)):
                # æ—¶é—´åµŒå…¥
                t_embed = torch.full((B, 1), t / self.n_diffusion_steps, device=self.device)
                inp = torch.cat([state, x, t_embed], dim=-1)
                
                # é¢„æµ‹å™ªå£°
                noise_pred = self.policy_net(inp)
                
                # å»å™ª
                alpha = self.alphas[t]
                alpha_cumprod = self.alphas_cumprod[t]
                beta = self.betas[t]
                
                x = (1 / torch.sqrt(alpha)) * (
                    x - (beta / torch.sqrt(1 - alpha_cumprod)) * noise_pred
                )
                
                if t > 0:
                    x = x + torch.sqrt(beta) * torch.randn_like(x)
            
            return x
        
        def _ref_sample_action_dist(self, state: torch.Tensor) -> torch.Tensor:
            """å‚è€ƒç­–ç•¥çš„æ‰©æ•£é‡‡æ ·åŠ¨ä½œåˆ†å¸ƒ"""
            B = state.shape[0]
            x = torch.randn(B, self.action_dim, device=self.device)
            
            for t in reversed(range(self.n_diffusion_steps)):
                t_embed = torch.full((B, 1), t / self.n_diffusion_steps, device=self.device)
                inp = torch.cat([state, x, t_embed], dim=-1)
                noise_pred = self.ref_policy(inp)
                
                alpha = self.alphas[t]
                alpha_cumprod = self.alphas_cumprod[t]
                beta = self.betas[t]
                
                x = (1 / torch.sqrt(alpha)) * (
                    x - (beta / torch.sqrt(1 - alpha_cumprod)) * noise_pred
                )
                
                if t > 0:
                    x = x + torch.sqrt(beta) * torch.randn_like(x)
            
            return x
        
        def select_action(self, state: np.ndarray, explore: bool = True) -> int:
            if explore and random.random() < self.epsilon:
                return random.randint(0, self.action_dim - 1)
            
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                action_logits = self._sample_action_dist(state_tensor)
                return action_logits.argmax(dim=-1).item()
        
        def update(self, batch: List) -> Dict[str, float]:
            if len(batch) < self.batch_size:
                return {}
            
            # æ•°æ®ç§»åˆ°GPU
            states = torch.FloatTensor(np.array([e.state for e in batch])).to(self.device)
            actions = torch.LongTensor([e.action for e in batch]).to(self.device)
            rewards = torch.FloatTensor([e.reward for e in batch]).to(self.device)
            next_states = torch.FloatTensor(np.array([e.next_state for e in batch])).to(self.device)
            dones = torch.FloatTensor([e.done for e in batch]).to(self.device)
            
            # === Qå­¦ä¹  ===
            q_values = self.q_network(states)
            current_q = q_values.gather(1, actions.unsqueeze(1)).squeeze()
            
            with torch.no_grad():
                next_q = self.target_q(next_states).max(dim=1)[0]
                target_q = rewards + self.gamma * next_q * (1 - dones)
            
            q_loss = F.mse_loss(current_q, target_q)
            
            # === è®¡ç®—ä¼˜åŠ¿ (ä½¿ç”¨æ­£ç¡®çš„TDè¯¯å·®) ===
            with torch.no_grad():
                # ä¿®å¤ï¼šä½¿ç”¨TDç›®æ ‡å‡å»å½“å‰Qå€¼ä½œä¸ºä¼˜åŠ¿ä¼°è®¡
                advantages = target_q - current_q.detach()
                # ä¼˜åŠ¿å½’ä¸€åŒ–ï¼Œç¡®ä¿æ¢¯åº¦ç¨³å®š
                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
                # ä¿®å¤ï¼šå°†ä¼˜åŠ¿é™åˆ¶åœ¨åˆç†èŒƒå›´ï¼Œé¿å…æç«¯å€¼å¯¼è‡´è´Ÿå¥–åŠ±
                advantages = torch.clamp(advantages, -5.0, 5.0)
            
            # === æ‰©æ•£ç­–ç•¥æ›´æ–° ===
            action_logits = self._sample_action_dist(states)
            
            # ä¿®å¤ï¼šä½¿ç”¨softmaxæ¦‚ç‡è€Œä¸æ˜¯ç›´æ¥log_softmaxï¼Œé¿å…æ•°å€¼é—®é¢˜
            action_probs = F.softmax(action_logits, dim=-1)
            log_probs = torch.log(action_probs + 1e-8)
            action_log_probs = log_probs.gather(1, actions.unsqueeze(1)).squeeze()
            
            # ç­–ç•¥æ¢¯åº¦æŸå¤±ï¼ˆä¼˜åŠ¿åŠ æƒï¼‰
            # ä¿®å¤ï¼šç¡®ä¿æ­£çš„ä¼˜åŠ¿å¯¹åº”æ­£çš„å¥–åŠ±æ–¹å‘
            pg_loss = -(action_log_probs * advantages.detach()).mean()
            
            # === é•œåƒä¸‹é™KLçº¦æŸ ===
            # ä½¿ç”¨ç›¸åŒçš„æ‰©æ•£é‡‡æ ·å¾—åˆ°å‚è€ƒç­–ç•¥è¾“å‡º
            with torch.no_grad():
                ref_action_logits = self._ref_sample_action_dist(states)
                ref_probs = F.softmax(ref_action_logits, dim=-1)
            
            # ä¿®å¤ï¼šä½¿ç”¨log_target=Trueé¿å…è´ŸKLæ•£åº¦
            kl_div = F.kl_div(log_probs, ref_probs, reduction='batchmean', log_target=False)
            # ç¡®ä¿KLä¸ä¸ºè´Ÿ
            kl_div = torch.abs(kl_div)
            
            # æ€»ç­–ç•¥æŸå¤± (é™ä½KLç³»æ•°ï¼Œè®©ç­–ç•¥æ›´è‡ªç”±æ¢ç´¢)
            policy_loss = pg_loss + self.kl_coef * 0.5 * kl_div
            
            # === æ€»æŸå¤± ===
            loss = q_loss + policy_loss
            
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(
                list(self.policy_net.parameters()) + list(self.q_network.parameters()), 
                1.0
            )
            self.optimizer.step()
            
            # === è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ ===
            for param, target_param in zip(self.q_network.parameters(), 
                                          self.target_q.parameters()):
                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
            
            # === å®šæœŸæ›´æ–°å‚è€ƒç­–ç•¥ï¼ˆæ¯100æ­¥åŒæ­¥ä¸€æ¬¡ï¼Œé˜²æ­¢KLè¿‡å¤§ï¼‰ ===
            if self.training_step % 100 == 0:
                self.ref_policy.load_state_dict(self.policy_net.state_dict())
            
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
            self.training_step += 1
            
            return {
                'loss': loss.item(),
                'q_loss': q_loss.item(),
                'pg_loss': pg_loss.item(),
                'kl_div': kl_div.item(),
                'advantage_mean': advantages.mean().item()
            }
        
        def save(self, path: str):
            torch.save({
                'policy_net': self.policy_net.state_dict(),
                'ref_policy': self.ref_policy.state_dict(),
                'q_network': self.q_network.state_dict(),
                'target_q': self.target_q.state_dict(),
                'epsilon': self.epsilon,
                'training_step': self.training_step
            }, path)
        
        def load(self, path: str):
            checkpoint = torch.load(path, map_location=self.device)
            self.policy_net.load_state_dict(checkpoint['policy_net'])
            self.ref_policy.load_state_dict(checkpoint['ref_policy'])
            self.q_network.load_state_dict(checkpoint['q_network'])
            self.target_q.load_state_dict(checkpoint['target_q'])
            self.epsilon = checkpoint['epsilon']
            self.training_step = checkpoint['training_step']


# ============================================================
# ç®—æ³•æ³¨å†Œä¸å·¥å‚
# ============================================================

ADVANCED_ALGORITHM_INFO = {
    'DiffusionPolicy': {
        'name': 'Diffusion Policy',
        'year': 2024,
        'venue': 'RSS/CoRL',
        'authors': 'Chi et al.',
        'type': 'Generative',
        'description': 'æ‰©æ•£æ¨¡å‹ç”ŸæˆåŠ¨ä½œï¼Œæ”¯æŒå¤šæ¨¡æ€åˆ†å¸ƒï¼Œæœºå™¨äººæ“ä½œSOTA'
    },
    'TDMPC2': {
        'name': 'TD-MPC2',
        'year': 2024,
        'venue': 'ICLR',
        'authors': 'Hansen et al.',
        'type': 'Model-based',
        'description': 'ä¸–ç•Œæ¨¡å‹+TDå­¦ä¹ +MPCè§„åˆ’ï¼Œå¤§è§„æ¨¡è¿ç»­æ§åˆ¶SOTA'
    },
    'MambaPolicy': {
        'name': 'Mamba Policy',
        'year': 2025,
        'venue': 'Emerging',
        'authors': 'Based on Gu & Dao',
        'type': 'Sequence Model',
        'description': 'é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ŒO(L)å¤æ‚åº¦ï¼Œå®æ—¶æ§åˆ¶å‹å¥½'
    },
    'DPMD': {
        'name': 'DPMD',
        'year': 2025,
        'venue': 'Emerging',
        'authors': 'Novel',
        'type': 'Generative + Optimization',
        'description': 'æ‰©æ•£ç­–ç•¥+é•œåƒä¸‹é™ï¼Œç¨³å®šæ›´æ–°ï¼ŒKLçº¦æŸé˜²å´©æºƒ'
    }
}


def get_advanced_algorithm(name: str, state_dim: int, action_dim: int, config: Dict = None):
    """
    è·å–2024-2025å¹´æ–°RLç®—æ³•å®ä¾‹
    
    Args:
        name: ç®—æ³•åç§° ('DiffusionPolicy', 'TDMPC2', 'MambaPolicy', 'DPMD')
        state_dim: çŠ¶æ€ç»´åº¦
        action_dim: åŠ¨ä½œç»´åº¦
        config: ç®—æ³•é…ç½®ï¼ˆåŒ…æ‹¬deviceç­‰ï¼‰
    
    Returns:
        RLç®—æ³•å®ä¾‹
    """
    if not TORCH_AVAILABLE:
        raise RuntimeError("PyTorch is required for advanced RL algorithms")
    
    algorithms = {
        'DiffusionPolicy': DiffusionPolicy,
        'Diffusion': DiffusionPolicy,
        'TDMPC2': TDMPC2,
        'TD-MPC2': TDMPC2,
        'MambaPolicy': MambaPolicy,
        'Mamba': MambaPolicy,
        'DPMD': DPMD,
    }
    
    if name not in algorithms:
        raise ValueError(f"Unknown algorithm: {name}. Available: {list(algorithms.keys())}")
    
    return algorithms[name](state_dim, action_dim, config or {})


def list_advanced_algorithms() -> List[str]:
    """è¿”å›å¯ç”¨çš„2024-2025æ–°ç®—æ³•åˆ—è¡¨"""
    return ['DiffusionPolicy', 'TDMPC2', 'MambaPolicy', 'DPMD']


def print_advanced_algorithms():
    """æ‰“å°æ‰€æœ‰æ–°ç®—æ³•ä¿¡æ¯"""
    print("\n" + "="*70)
    print("2024-2025å¹´æœ€æ–°å¼ºåŒ–å­¦ä¹ ç®—æ³•")
    print("="*70)
    for name, info in ADVANCED_ALGORITHM_INFO.items():
        print(f"\nğŸ“Œ {info['name']} ({name})")
        print(f"   å¹´ä»½: {info['year']} | æ¥æº: {info['venue']}")
        print(f"   ç±»å‹: {info['type']}")
        print(f"   æè¿°: {info['description']}")
    print("\n" + "="*70)
