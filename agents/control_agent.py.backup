"""
控制智能体
==========
基于DQN强化学习的主动容错控制智能体

创新点:
1. DQN策略网络: 学习最优VIT和燃油调整策略
2. 安全约束层: 硬约束确保控制动作不违反物理限制
3. 混合控制: RL策略 + PID兜底的分层架构
4. 在线微调: 支持部署后的在线学习
"""

import numpy as np
from dataclasses import dataclass, field
from typing import Dict, Any, List, Tuple, Optional
from enum import Enum, auto
from collections import deque
import warnings

from .base_agent import Agent, AgentMessage, MessageType, ReplayBuffer

# 尝试导入PyTorch
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    nn = None
    optim = None
    F = None
    warnings.warn("PyTorch not available, using PID controller only")

import sys
sys.path.append('..')
from diagnosis.fault_injector import FaultType


class ControlMode(Enum):
    """控制模式"""
    NORMAL = auto()
    FAULT_TOLERANT = auto()
    DEGRADED = auto()
    EMERGENCY = auto()


@dataclass
class ControlAction:
    """控制动作"""
    timestamp: float
    mode: ControlMode
    vit_adjustment: float = 0.0
    fuel_adjustment: float = 0.0
    speed_target: float = 0.0
    power_limit: float = 1.0
    cylinder_mask: List[bool] = field(default_factory=list)
    message: str = ""
    action_source: str = "PID"  # "PID" or "RL"


# 仅在PyTorch可用时定义DQN相关类
if TORCH_AVAILABLE:
    class DQNNetwork(nn.Module):
        """
        DQN深度Q网络
        
        网络架构:
        - 输入: 状态向量 [Pmax, Pcomp, Texh, residuals, mode, vit_current, fuel_current]
        - 隐藏层: 2层全连接 (128, 64)
        - 输出: Q值 (离散动作空间)
        """
        
        def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
            super(DQNNetwork, self).__init__()
            
            self.fc1 = nn.Linear(state_dim, hidden_dim)
            self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)
            self.fc3 = nn.Linear(hidden_dim // 2, action_dim)
            
            # 初始化权重
            self._init_weights()
        
        def _init_weights(self):
            for m in self.modules():
                if isinstance(m, nn.Linear):
                    nn.init.xavier_uniform_(m.weight)
                    nn.init.constant_(m.bias, 0)
        
        def forward(self, x):
            x = F.relu(self.fc1(x))
            x = F.relu(self.fc2(x))
            return self.fc3(x)


    class DQNController:
        """
        DQN控制器
        
        实现:
        - 双网络架构 (online + target)
        - 经验回放
        - ε-greedy探索
        """
        
        def __init__(self, state_dim: int = 10, n_vit_actions: int = 9, 
                 n_fuel_actions: int = 5, device: str = 'cpu'):
        """
        Args:
            state_dim: 状态维度
            n_vit_actions: VIT离散动作数 (如 -4, -3, -2, -1, 0, 1, 2, 3, 4)
            n_fuel_actions: 燃油离散动作数 (如 0.7, 0.8, 0.9, 0.95, 1.0)
            device: 计算设备
        """
        self.state_dim = state_dim
        self.n_vit_actions = n_vit_actions
        self.n_fuel_actions = n_fuel_actions
        self.action_dim = n_vit_actions * n_fuel_actions  # 组合动作空间
        
        self.device = device
        
        # VIT动作映射 [-8, -6, -4, -2, 0, 2, 4] 度
        self.vit_actions = np.linspace(-8, 4, n_vit_actions)
        
        # 燃油动作映射 [0.7, 0.8, 0.9, 0.95, 1.0]
        self.fuel_actions = np.linspace(0.7, 1.0, n_fuel_actions)
        
        # 网络
        self.online_net = DQNNetwork(state_dim, self.action_dim).to(device)
        self.target_net = DQNNetwork(state_dim, self.action_dim).to(device)
        self.target_net.load_state_dict(self.online_net.state_dict())
        
        # 优化器
        self.optimizer = optim.Adam(self.online_net.parameters(), lr=0.001)
        
        # 经验回放
        self.replay_buffer = ReplayBuffer(capacity=10000)
        
        # 超参数
        self.gamma = 0.99          # 折扣因子
        self.epsilon = 1.0         # 探索率
        self.epsilon_min = 0.05
        self.epsilon_decay = 0.995
        self.batch_size = 64
        self.target_update_freq = 100
        
        # 训练计数
        self.train_step = 0
        self.is_trained = False
    
    def encode_state(self, observation: Dict[str, Any], 
                     diagnosis_result: Any,
                     current_vit: float,
                     current_fuel: float) -> np.ndarray:
        """
        编码状态向量
        
        Args:
            observation: 测量值
            diagnosis_result: 诊断结果
            current_vit: 当前VIT值
            current_fuel: 当前燃油系数
            
        Returns:
            state: 归一化状态向量
        """
        # 提取特征
        Pmax = observation.get('Pmax', 170) / 200.0  # 归一化
        Pcomp = observation.get('Pcomp', 150) / 200.0
        Texh = observation.get('Texh', 350) / 500.0
        
        residuals = diagnosis_result.residuals if diagnosis_result else {}
        r_Pmax = residuals.get('Pmax', 0)
        r_Pcomp = residuals.get('Pcomp', 0)
        r_Texh = residuals.get('Texh', 0)
        
        # 故障类型编码
        fault_type_enc = 0
        if diagnosis_result and diagnosis_result.fault_detected:
            fault_type_enc = hash(diagnosis_result.fault_type.name) % 10 / 10.0
        
        # 模式编码
        mode_enc = 0
        if diagnosis_result:
            mode_map = {
                'HEALTHY': 0,
                'WARNING': 0.33,
                'FAULT': 0.66,
                'CRITICAL': 1.0
            }
            mode_enc = mode_map.get(diagnosis_result.diagnosis_state.name, 0)
        
        # VIT和燃油归一化
        vit_norm = (current_vit + 8) / 12.0  # [-8, 4] -> [0, 1]
        fuel_norm = (current_fuel - 0.7) / 0.3  # [0.7, 1.0] -> [0, 1]
        
        state = np.array([
            Pmax, Pcomp, Texh,
            r_Pmax, r_Pcomp, r_Texh,
            fault_type_enc, mode_enc,
            vit_norm, fuel_norm
        ], dtype=np.float32)
        
        return state
    
    def decode_action(self, action_idx: int) -> Tuple[float, float]:
        """
        解码动作索引为VIT和燃油值
        
        Args:
            action_idx: 组合动作索引
            
        Returns:
            (vit_value, fuel_value)
        """
        vit_idx = action_idx // self.n_fuel_actions
        fuel_idx = action_idx % self.n_fuel_actions
        
        return self.vit_actions[vit_idx], self.fuel_actions[fuel_idx]
    
    def select_action(self, state: np.ndarray, training: bool = True) -> int:
        """
        选择动作 (ε-greedy策略)
        
        Args:
            state: 状态向量
            training: 是否处于训练模式
            
        Returns:
            action_idx: 动作索引
        """
        if training and np.random.random() < self.epsilon:
            # 探索
            return np.random.randint(self.action_dim)
        else:
            # 利用
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                q_values = self.online_net(state_tensor)
                return q_values.argmax().item()
    
    def compute_reward(self, observation: Dict[str, float],
                       vit: float, fuel: float,
                       Pmax_limit: float = 190.0) -> float:
        """
        计算奖励
        
        奖励函数设计:
        - 安全奖励: Pmax在安全范围内得正奖励
        - 效率惩罚: 燃油削减和VIT调整会有代价
        - 稳定性奖励: 平稳控制得奖励
        """
        Pmax = observation.get('Pmax', 170)
        
        reward = 0.0
        
        # 安全奖励 (最重要)
        if Pmax <= Pmax_limit:
            # 在安全范围内，越接近目标越好
            target_Pmax = 170.0
            deviation = abs(Pmax - target_Pmax) / target_Pmax
            reward += 1.0 - deviation  # [0, 1]
        else:
            # 超限惩罚
            overshoot = (Pmax - Pmax_limit) / Pmax_limit
            reward -= 5.0 * overshoot  # 强惩罚
        
        # 效率惩罚
        fuel_penalty = (1.0 - fuel) * 0.5  # 燃油削减的代价
        vit_penalty = abs(vit) * 0.02      # VIT调整的代价
        reward -= (fuel_penalty + vit_penalty)
        
        # 平稳控制奖励 (避免剧烈波动)
        # TODO: 需要历史数据计算
        
        return reward
    
    def update(self) -> Optional[float]:
        """
        执行一次DQN更新
        
        Returns:
            loss: 训练损失 (如果执行了训练)
        """
        if len(self.replay_buffer) < self.batch_size:
            return None
        
        # 采样batch
        batch = self.replay_buffer.sample(self.batch_size)
        
        states = torch.FloatTensor(np.array([e['state'] for e in batch])).to(self.device)
        actions = torch.LongTensor([e['action'] for e in batch]).to(self.device)
        rewards = torch.FloatTensor([e['reward'] for e in batch]).to(self.device)
        next_states = torch.FloatTensor(np.array([e['next_state'] for e in batch])).to(self.device)
        dones = torch.FloatTensor([e['done'] for e in batch]).to(self.device)
        
        # 当前Q值
        current_q = self.online_net(states).gather(1, actions.unsqueeze(1))
        
        # 目标Q值 (Double DQN)
        with torch.no_grad():
            next_actions = self.online_net(next_states).argmax(1, keepdim=True)
            next_q = self.target_net(next_states).gather(1, next_actions)
            target_q = rewards.unsqueeze(1) + self.gamma * next_q * (1 - dones.unsqueeze(1))
        
        # 损失
        loss = F.mse_loss(current_q, target_q)
        
        # 优化
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.online_net.parameters(), 1.0)
        self.optimizer.step()
        
        # 更新目标网络
        self.train_step += 1
        if self.train_step % self.target_update_freq == 0:
            self.target_net.load_state_dict(self.online_net.state_dict())
        
        # 衰减探索率
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
        
        self.is_trained = True
        
        return loss.item()
    
    def save(self, path: str) -> None:
        """保存模型"""
        torch.save({
            'online_net': self.online_net.state_dict(),
            'target_net': self.target_net.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'train_step': self.train_step,
        }, path)
    
    def load(self, path: str) -> None:
        """加载模型"""
        checkpoint = torch.load(path, map_location=self.device)
        self.online_net.load_state_dict(checkpoint['online_net'])
        self.target_net.load_state_dict(checkpoint['target_net'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.epsilon = checkpoint['epsilon']
        self.train_step = checkpoint['train_step']
        self.is_trained = True


class PIDController:
    """
    PID控制器 (作为安全兜底)
    """
    
    def __init__(self, Kp: float = 0.5, Ki: float = 0.05, Kd: float = 0.1):
        self.Kp = Kp
        self.Ki = Ki
        self.Kd = Kd
        
        self.integral = 0.0
        self.last_error = 0.0
    
    def compute(self, error: float, dt: float = 0.1) -> float:
        """计算PID输出"""
        self.integral += error * dt
        self.integral = np.clip(self.integral, -10, 10)  # 积分限幅
        
        derivative = (error - self.last_error) / dt
        self.last_error = error
        
        output = self.Kp * error + self.Ki * self.integral + self.Kd * derivative
        return output
    
    def reset(self):
        self.integral = 0.0
        self.last_error = 0.0


class ControlAgent(Agent):
    """
    控制智能体
    
    分层架构:
    1. 高层: RL策略 (DQN) 决定VIT和燃油调整
    2. 低层: PID控制器作为安全兜底
    3. 安全层: 硬约束检查
    """
    
    def __init__(self, engine, name: str = "ControlAgent", 
                 use_rl: bool = True, device: str = 'cpu'):
        """
        Args:
            engine: 发动机模型
            name: 智能体名称
            use_rl: 是否使用RL控制器
            device: 计算设备
        """
        super().__init__(name=name, engine=engine)
        
        self.use_rl = use_rl and TORCH_AVAILABLE
        
        # 发动机参数
        self.n_cylinders = engine.geometry.n_cylinders
        
        # 安全限值
        self.Pmax_limit = 190.0
        self.Pmax_warning = 180.0
        self.Pmax_target = 170.0
        
        # 控制范围
        self.vit_min = -8.0
        self.vit_max = 4.0
        self.fuel_min = 0.7
        self.fuel_max = 1.0
        
        # 当前状态
        self.vit_current = 0.0
        self.fuel_current = 1.0
        self.mode = ControlMode.NORMAL
        
        # 各缸状态
        self.cylinder_load = [1.0] * self.n_cylinders
        self.cylinder_healthy = [True] * self.n_cylinders
        
        # 控制器
        if self.use_rl:
            self.dqn = DQNController(
                state_dim=10,
                n_vit_actions=9,
                n_fuel_actions=5,
                device=device
            )
        
        self.pid_vit = PIDController(Kp=0.5, Ki=0.05, Kd=0.1)
        self.pid_fuel = PIDController(Kp=0.3, Ki=0.02, Kd=0.05)
        
        # 最近的诊断结果
        self.last_diagnosis = None
        
        # 控制历史
        self.control_history: List[ControlAction] = []
        
        # 上一状态 (用于RL学习)
        self.last_state = None
        self.last_action = None
        
        # 性能指标
        self.state.performance_metrics = {
            'total_actions': 0,
            'rl_actions': 0,
            'pid_actions': 0,
            'pmax_violations': 0,
            'avg_reward': 0.0,
        }
    
    def perceive(self, observation: Dict[str, Any]) -> Dict[str, Any]:
        """
        感知：获取当前状态
        """
        # 从消息队列获取诊断结果
        for msg in self.inbox:
            if msg.msg_type == MessageType.DIAGNOSIS_RESULT:
                self.last_diagnosis = msg.payload.get('result')
        
        # 清空收件箱
        self.inbox.clear()
        
        # 构建状态表示
        if self.use_rl:
            state = self.dqn.encode_state(
                observation,
                self.last_diagnosis,
                self.vit_current,
                self.fuel_current
            )
        else:
            state = None
        
        return {
            'observation': observation,
            'diagnosis': self.last_diagnosis,
            'rl_state': state,
            'current_vit': self.vit_current,
            'current_fuel': self.fuel_current,
        }
    
    def decide(self, perception: Dict[str, Any]) -> Dict[str, Any]:
        """
        决策：选择控制动作
        """
        observation = perception['observation']
        diagnosis = perception['diagnosis']
        Pmax = observation.get('Pmax', self.Pmax_target)
        
        # 更新控制模式
        self._update_mode(diagnosis, observation)
        
        # 根据模式选择控制策略
        if self.mode == ControlMode.EMERGENCY:
            # 紧急模式: 直接使用硬编码策略
            decision = self._emergency_decision()
        elif self.mode == ControlMode.DEGRADED:
            # 降级模式: 切缸 + 负荷重分配
            decision = self._degraded_decision(diagnosis)
        else:
            # 正常/容错模式: RL or PID
            if self.use_rl and self.dqn.is_trained:
                decision = self._rl_decision(perception)
            else:
                decision = self._pid_decision(observation)
        
        # 安全约束检查
        decision = self._apply_safety_constraints(decision, observation)
        
        return decision
    
    def act(self, decision: Dict[str, Any]) -> Dict[str, Any]:
        """
        执行：应用控制动作
        """
        vit = decision['vit']
        fuel = decision['fuel']
        source = decision['source']
        
        # 更新当前值
        self.vit_current = vit
        self.fuel_current = fuel
        
        # 应用到发动机
        base_timing = self.engine.calibrated_params.get('injection_timing', 2.0)
        self.engine.set_injection_timing(base_timing + vit)
        
        # 创建控制动作
        action = ControlAction(
            timestamp=self.state.last_update,
            mode=self.mode,
            vit_adjustment=vit,
            fuel_adjustment=(fuel - 1.0) * 100,
            power_limit=fuel,
            cylinder_mask=self.cylinder_healthy.copy(),
            message=self._generate_message(decision),
            action_source=source
        )
        
        # 记录历史
        self.control_history.append(action)
        
        # 更新统计
        self.state.performance_metrics['total_actions'] += 1
        if source == 'RL':
            self.state.performance_metrics['rl_actions'] += 1
        else:
            self.state.performance_metrics['pid_actions'] += 1
        
        # 发送控制动作消息
        self.send_message(
            msg_type=MessageType.CONTROL_ACTION,
            receiver="Coordinator",
            payload={'action': action},
            priority=7
        )
        
        return {
            'action': action,
            'mode': self.mode,
        }
    
    def learn_from_experience(self, observation: Dict[str, float], 
                               reward: Optional[float] = None) -> Optional[float]:
        """
        从经验中学习 (在线微调)
        
        Args:
            observation: 当前观测
            reward: 外部提供的奖励 (如果为None则自动计算)
            
        Returns:
            loss: 训练损失
        """
        if not self.use_rl:
            return None
        
        # 计算奖励
        if reward is None:
            reward = self.dqn.compute_reward(
                observation,
                self.vit_current,
                self.fuel_current,
                self.Pmax_limit
            )
        
        # 编码当前状态
        current_state = self.dqn.encode_state(
            observation,
            self.last_diagnosis,
            self.vit_current,
            self.fuel_current
        )
        
        # 存储经验
        if self.last_state is not None and self.last_action is not None:
            done = (self.mode == ControlMode.EMERGENCY)
            self.dqn.replay_buffer.push(
                self.last_state,
                self.last_action,
                reward,
                current_state,
                done
            )
        
        # 更新网络
        loss = self.dqn.update()
        
        # 更新统计
        if loss is not None:
            old_avg = self.state.performance_metrics['avg_reward']
            count = self.state.performance_metrics['total_actions']
            self.state.performance_metrics['avg_reward'] = (old_avg * count + reward) / (count + 1)
        
        return loss
    
    def _update_mode(self, diagnosis, observation: Dict[str, float]):
        """更新控制模式"""
        Pmax = observation.get('Pmax', 0)
        
        # 紧急模式
        if Pmax > self.Pmax_limit:
            self.mode = ControlMode.EMERGENCY
            self.state.performance_metrics['pmax_violations'] += 1
            return
        
        # 基于诊断结果
        if diagnosis and diagnosis.fault_detected:
            if diagnosis.fault_type == FaultType.CYLINDER_LEAK:
                self.mode = ControlMode.DEGRADED
            else:
                self.mode = ControlMode.FAULT_TOLERANT
            return
        
        # Pmax预警
        if Pmax > self.Pmax_warning:
            self.mode = ControlMode.FAULT_TOLERANT
            return
        
        self.mode = ControlMode.NORMAL
    
    def _rl_decision(self, perception: Dict[str, Any]) -> Dict[str, Any]:
        """RL决策"""
        state = perception['rl_state']
        
        # 选择动作
        action_idx = self.dqn.select_action(state, training=self.learning_enabled)
        vit, fuel = self.dqn.decode_action(action_idx)
        
        # 保存用于学习
        self.last_state = state
        self.last_action = action_idx
        
        return {
            'vit': vit,
            'fuel': fuel,
            'source': 'RL',
            'action_idx': action_idx,
        }
    
    def _pid_decision(self, observation: Dict[str, float]) -> Dict[str, Any]:
        """PID决策"""
        Pmax = observation.get('Pmax', self.Pmax_target)
        error = Pmax - self.Pmax_target
        
        # VIT调整
        vit_delta = -self.pid_vit.compute(error)
        new_vit = np.clip(self.vit_current + vit_delta, self.vit_min, self.vit_max)
        
        # 燃油调整 (仅在VIT饱和时)
        new_fuel = self.fuel_current
        if new_vit <= self.vit_min and error > 5:
            fuel_delta = -self.pid_fuel.compute(error)
            new_fuel = np.clip(self.fuel_current + fuel_delta, self.fuel_min, self.fuel_max)
        
        return {
            'vit': new_vit,
            'fuel': new_fuel,
            'source': 'PID',
        }
    
    def _emergency_decision(self) -> Dict[str, Any]:
        """紧急决策"""
        return {
            'vit': self.vit_min,  # 最大滞后
            'fuel': max(self.fuel_current - 0.1, self.fuel_min),  # 快速减油
            'source': 'EMERGENCY',
        }
    
    def _degraded_decision(self, diagnosis) -> Dict[str, Any]:
        """降级决策"""
        # 识别故障缸 (简化：假设第1缸)
        faulty_cylinder = 0
        
        # 切缸
        self.cylinder_healthy[faulty_cylinder] = False
        self.cylinder_load[faulty_cylinder] = 0.0
        
        # 负荷重分配
        healthy_count = sum(self.cylinder_healthy)
        if healthy_count > 0:
            extra_load = 1.0 / healthy_count
            for i in range(self.n_cylinders):
                if self.cylinder_healthy[i]:
                    self.cylinder_load[i] = min(1.0 + extra_load * 0.15, 1.15)
        
        return {
            'vit': self.vit_current,
            'fuel': 0.9,  # 降低到90%
            'source': 'DEGRADED',
        }
    
    def _apply_safety_constraints(self, decision: Dict[str, Any], 
                                   observation: Dict[str, float]) -> Dict[str, Any]:
        """应用安全约束"""
        vit = decision['vit']
        fuel = decision['fuel']
        
        # 硬约束
        vit = np.clip(vit, self.vit_min, self.vit_max)
        fuel = np.clip(fuel, self.fuel_min, self.fuel_max)
        
        # 如果Pmax严重超限，强制使用保守策略
        Pmax = observation.get('Pmax', 0)
        if Pmax > self.Pmax_limit + 5:  # 超过5bar
            vit = self.vit_min
            fuel = max(fuel - 0.1, self.fuel_min)
            decision['source'] = 'SAFETY_OVERRIDE'
        
        decision['vit'] = vit
        decision['fuel'] = fuel
        
        return decision
    
    def _generate_message(self, decision: Dict[str, Any]) -> str:
        """生成控制消息"""
        mode_str = self.mode.name
        source = decision['source']
        vit = decision['vit']
        fuel = decision['fuel']
        
        if self.mode == ControlMode.NORMAL:
            return f"正常运行 [{source}]"
        elif self.mode == ControlMode.FAULT_TOLERANT:
            return f"容错控制: VIT={vit:+.1f}°, 燃油={fuel*100:.0f}% [{source}]"
        elif self.mode == ControlMode.DEGRADED:
            healthy = sum(self.cylinder_healthy)
            return f"降级运行: {healthy}/{self.n_cylinders}缸工作 [{source}]"
        else:
            return f"⚠️ 紧急保护: VIT={vit:+.1f}°, 燃油={fuel*100:.0f}% [{source}]"
    
    def update(self, Y_measured: Dict[str, float], 
               timestamp: float = 0.0) -> ControlAction:
        """
        便捷接口：执行完整控制流程
        
        Args:
            Y_measured: 测量值
            timestamp: 时间戳
            
        Returns:
            ControlAction
        """
        result = self.step(Y_measured, timestamp)
        
        # 在线学习
        if self.learning_enabled:
            self.learn_from_experience(Y_measured)
        
        return result['action']
    
    def save_model(self, path: str) -> None:
        """保存RL模型"""
        if self.use_rl:
            self.dqn.save(path)
    
    def load_model(self, path: str) -> None:
        """加载RL模型"""
        if self.use_rl:
            self.dqn.load(path)
    
    def reset(self) -> None:
        """重置智能体"""
        super().reset()
        self.vit_current = 0.0
        self.fuel_current = 1.0
        self.mode = ControlMode.NORMAL
        self.cylinder_load = [1.0] * self.n_cylinders
        self.cylinder_healthy = [True] * self.n_cylinders
        self.pid_vit.reset()
        self.pid_fuel.reset()
        self.last_diagnosis = None
        self.last_state = None
        self.last_action = None
        self.control_history.clear()
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """获取性能汇总"""
        return {
            'mode': self.mode.name,
            'current_vit': self.vit_current,
            'current_fuel': self.fuel_current,
            'rl_trained': self.dqn.is_trained if self.use_rl else False,
            'epsilon': self.dqn.epsilon if self.use_rl else None,
            **self.state.performance_metrics
        }
