#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿæµ‹è¯•è„šæœ¬ - éªŒè¯æ‰€æœ‰ç®—æ³•å®žçŽ°æ­£ç¡®æ€§
æ¯ä¸ªç®—æ³•åªè®­ç»ƒ10ä¸ªepisode
"""

import sys
import os
import time
import numpy as np
import random

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def test_all_algorithms():
    """æµ‹è¯•æ‰€æœ‰ç®—æ³•"""
    print("="*60)
    print("ðŸ§ª å¿«é€Ÿç®—æ³•éªŒè¯æµ‹è¯• (10 episodes)")
    print("="*60)
    
    # æ£€æŸ¥PyTorch
    try:
        import torch
        print(f"PyTorch: {torch.__version__}")
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"Device: {device}")
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return
    
    # åˆ›å»ºç®€å•çŽ¯å¢ƒ
    from experiments.five_method_comparison import DieselEngineEnv, PIDController
    
    # æµ‹è¯•é…ç½®
    state_dim = 8
    action_dim = 5
    n_episodes = 10
    max_steps = 100
    
    results = {}
    
    # 1. PID
    print("\n--- æµ‹è¯• PID ---")
    try:
        env = DieselEngineEnv(seed=42)
        agent = PIDController()
        reward = test_agent(agent, env, n_episodes, max_steps)
        results['PID'] = reward
        print(f"âœ… PID: å¹³å‡å¥–åŠ± = {reward:.2f}")
    except Exception as e:
        print(f"âŒ PIDå¤±è´¥: {e}")
    
    # 2. SAC
    print("\n--- æµ‹è¯• SAC ---")
    try:
        from agents.rl_algorithms import get_algorithm
        env = DieselEngineEnv(seed=42)
        agent = get_algorithm("SAC", state_dim, action_dim, {'device': device})
        reward = test_agent(agent, env, n_episodes, max_steps, train=True)
        results['SAC'] = reward
        print(f"âœ… SAC: å¹³å‡å¥–åŠ± = {reward:.2f}")
    except Exception as e:
        print(f"âŒ SACå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # 3. TD-MPC2
    print("\n--- æµ‹è¯• TD-MPC2 ---")
    try:
        from agents.advanced_rl_algorithms import get_advanced_algorithm
        env = DieselEngineEnv(seed=42)
        agent = get_advanced_algorithm("TDMPC2", state_dim, action_dim, {'device': device})
        reward = test_agent(agent, env, n_episodes, max_steps, train=True)
        results['TDMPC2'] = reward
        print(f"âœ… TD-MPC2: å¹³å‡å¥–åŠ± = {reward:.2f}")
    except Exception as e:
        print(f"âŒ TD-MPC2å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # 4. Mamba Policy
    print("\n--- æµ‹è¯• Mamba Policy ---")
    try:
        from agents.advanced_rl_algorithms import get_advanced_algorithm
        env = DieselEngineEnv(seed=42)
        agent = get_advanced_algorithm("MambaPolicy", state_dim, action_dim, {'device': device})
        reward = test_agent(agent, env, n_episodes, max_steps, train=True, is_mamba=True)
        results['MambaPolicy'] = reward
        print(f"âœ… Mamba Policy: å¹³å‡å¥–åŠ± = {reward:.2f}")
    except Exception as e:
        print(f"âŒ Mamba Policyå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # 5. DPMD
    print("\n--- æµ‹è¯• DPMD ---")
    try:
        from agents.advanced_rl_algorithms import get_advanced_algorithm
        env = DieselEngineEnv(seed=42)
        agent = get_advanced_algorithm("DPMD", state_dim, action_dim, {'device': device})
        reward = test_agent(agent, env, n_episodes, max_steps, train=True)
        results['DPMD'] = reward
        print(f"âœ… DPMD: å¹³å‡å¥–åŠ± = {reward:.2f}")
    except Exception as e:
        print(f"âŒ DPMDå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("ðŸ“Š æµ‹è¯•ç»“æžœæ±‡æ€»")
    print("="*60)
    for name, reward in results.items():
        status = "âœ…" if reward is not None else "âŒ"
        print(f"  {status} {name}: {reward:.2f if reward else 'FAILED'}")
    
    print(f"\næˆåŠŸ: {len(results)}/5 ç®—æ³•")
    print("="*60)
    
    return results


def test_agent(agent, env, n_episodes, max_steps, train=False, is_mamba=False):
    """æµ‹è¯•å•ä¸ªagent"""
    total_reward = 0
    
    for ep in range(n_episodes):
        state = env.reset()
        if hasattr(agent, 'reset'):
            agent.reset()
        if hasattr(agent, 'reset_history'):
            agent.reset_history()
        
        ep_reward = 0
        ep_states = []
        ep_actions = []
        ep_rewards = []
        
        for step in range(max_steps):
            action = agent.select_action(state, explore=True)
            next_state, reward, done, _ = env.step(action)
            
            ep_reward += reward
            ep_states.append(state)
            ep_actions.append(action)
            ep_rewards.append(reward)
            
            # å­˜å‚¨ç»éªŒ
            if train and hasattr(agent, 'buffer') and agent.buffer is not None:
                agent.buffer.push(state, action, reward, next_state, done)
                
                # æ›´æ–°
                if len(agent.buffer) >= 32:
                    batch = agent.buffer.sample(32)
                    agent.update(batch)
            
            state = next_state
            if done:
                break
        
        # Mambaè½¨è¿¹å­˜å‚¨
        if is_mamba and hasattr(agent, 'store_trajectory'):
            agent.store_trajectory(ep_states, ep_actions, ep_rewards)
        
        total_reward += ep_reward
    
    return total_reward / n_episodes


if __name__ == "__main__":
    test_all_algorithms()
